{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804dd343",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "from typing import List, Tuple, Literal, Any, Optional, cast, Callable\n",
    "from utils.agent import Agent\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.algorithm import AlgorithmInterface\n",
    "from utils.preprocess import PreprocessInterface\n",
    "import torch\n",
    "from collections import deque\n",
    "from torchvision import transforms\n",
    "import math\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from utils.common import Step, Episode, TransitionGeneric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba56515b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd146c29fb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d494e7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f93d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env.seed(RANDOM_SEED)\n",
    "env.reset()\n",
    "env._max_episode_steps = 100_0000\n",
    "TOTAL_ACTIONS = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c43d75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7746b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape is (210, 160, 3)\n",
    "Observation = npt.NDArray[np.uint8]\n",
    "Action = int\n",
    "\n",
    "# shape is (4, 210, 160, 3)\n",
    "State = torch.Tensor\n",
    "Reward = int\n",
    "\n",
    "Transition = TransitionGeneric[State, Action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e29eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        # self.first = nn.Sequential(nn.Conv2d(4, 32, (8, 8), 4), nn.ReLU()) self.second = nn.Sequential()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, (8, 8), 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (4, 4), 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, (3, 3), 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.Linear(512, TOTAL_ACTIONS),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x: State) -> torch.Tensor:\n",
    "        rlt = cast(torch.Tensor, self.net(x.to(device)))\n",
    "        return rlt.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37d4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAlgorithm(AlgorithmInterface[State, Action]):\n",
    "    def __init__(self):\n",
    "\n",
    "        self.times = 1\n",
    "        self.last_action = None\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def allowed_actions(self, state: State) -> List[Action]:\n",
    "        return list(range(TOTAL_ACTIONS))\n",
    "\n",
    "    def take_action(self, state: State) -> Action:\n",
    "        self.times += 1\n",
    "\n",
    "        if self.times % 10 == 0:\n",
    "            act = np.random.choice(self.allowed_actions(state))\n",
    "            self.last_action = act\n",
    "            return act\n",
    "\n",
    "        if self.last_action is not None:\n",
    "            return self.last_action\n",
    "\n",
    "        act = np.random.choice(self.allowed_actions(state))\n",
    "        self.last_action = act\n",
    "        return act\n",
    "\n",
    "    def after_step(\n",
    "        self,\n",
    "        sa: Tuple[State, Action],\n",
    "        episode: Episode[State, Action],\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    def on_termination(self, episode: Episode[State, Action]):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NNAlgorithm(AlgorithmInterface[State, Action]):\n",
    "    def __init__(self, nn: DQN, training_times: int = 50_00_0000, gamma: float = 0.99):\n",
    "        self.network = nn\n",
    "        self.optimizer = torch.optim.RMSprop(\n",
    "            self.network.parameters(), 1e-3, 0.95, 0.95, 1e-2\n",
    "        )\n",
    "\n",
    "        self.shrink = min(training_times / 50_00_0000, 1)\n",
    "        if self.shrink != 1:\n",
    "            print(f\"training on shrinked mode: {self.shrink}\")\n",
    "\n",
    "        self.target_network = DQN()\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "\n",
    "        self.times: int = 1\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.update_freq: int = 5\n",
    "        self.update_target = 200\n",
    "\n",
    "        self.memory_replay: deque[Transition] = deque(\n",
    "            maxlen=math.ceil(100_0000 * self.shrink)\n",
    "        )\n",
    "        self.gamma = gamma\n",
    "        self.loss_func = torch.nn.MSELoss()\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def allowed_actions(self, state: State) -> List[Action]:\n",
    "        return list(range(TOTAL_ACTIONS))\n",
    "\n",
    "    def take_action(self, state: State) -> Action:\n",
    "        rand = np.random.random()\n",
    "        max_decry_times = 100_0000 * self.shrink\n",
    "        sigma = -0.9 / max_decry_times * \\\n",
    "            np.min([self.times, max_decry_times]) + 1\n",
    "        if rand < sigma:\n",
    "            return np.random.choice(self.allowed_actions(state))\n",
    "        else:\n",
    "            act_vals: torch.Tensor = self.network(state)\n",
    "            maxi = torch.argmax(act_vals)\n",
    "            return cast(Action, maxi)\n",
    "\n",
    "    def after_step(\n",
    "        self,\n",
    "        sa: Tuple[State, Action],\n",
    "        episode: Episode[State, Action],\n",
    "    ):\n",
    "        (s, a, r) = episode[-1]\n",
    "        (sn, an) = sa\n",
    "        self.memory_replay.append(\n",
    "            (s, cast(Action, a), cast(float, r), sn, an))\n",
    "\n",
    "        if self.times % self.update_freq == 0 and len(self.memory_replay) >= 48:\n",
    "\n",
    "            batch: List[Transition] = []\n",
    "            for i in np.random.randint(0, len(self.memory_replay), 32):\n",
    "                batch.append(self.memory_replay[i])\n",
    "\n",
    "            self.train(batch)\n",
    "\n",
    "        if self.times % (self.update_target * self.update_freq) == 0:\n",
    "            self.update_target_network()\n",
    "\n",
    "        self.times += 1\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "\n",
    "    def clip_reward(self, r: float) -> float:\n",
    "        if r >= 1:\n",
    "            return 1.0\n",
    "        elif r <= -1:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def train(self, batch: List[Transition]):\n",
    "        # batch = torch.tensor(_b)\n",
    "        target = torch.tensor(\n",
    "            [\n",
    "                self.clip_reward(r)\n",
    "                if an is None\n",
    "                else self.clip_reward(r)\n",
    "                + self.gamma * torch.max(self.target_network(sn))\n",
    "                for (_, _, r, sn, an) in batch\n",
    "            ]\n",
    "        )\n",
    "        assert target.shape == (32,)\n",
    "        x = torch.cat([self.network(s)[:, a] for (s, a, _, _, _) in batch])\n",
    "\n",
    "        assert x.shape == (32,)\n",
    "\n",
    "        loss = self.loss_func(x, target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def extract_transitions(self, episode: Episode[State, Action]) -> List[Transition]:\n",
    "        trs: List[Transition] = []\n",
    "        for (idx, (s, a, r)) in enumerate(episode[:-1]):\n",
    "            (sn, an, _) = episode[idx + 1]\n",
    "            trs.append((s, cast(Action, a), cast(float, r), sn, an))\n",
    "        return trs\n",
    "\n",
    "    def on_termination(self, episode: Episode[State, Action]):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Preprocess(PreprocessInterface[Observation, Action, State]):\n",
    "    def __init__(self):\n",
    "        self.trfm: Callable[[Observation], State] = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Grayscale(),\n",
    "             transforms.Resize((84, 84))]\n",
    "        )\n",
    "        self.history: Episode[State, Action] = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.history = []\n",
    "\n",
    "    def get_current_state(self, h: Episode[Observation, Action]) -> State:\n",
    "        assert len(h) > 0\n",
    "\n",
    "        last_4_arr = self.stack_4(h, -1)\n",
    "\n",
    "        rlt = torch.stack([self.trfm(i)\n",
    "                          for i in last_4_arr]).squeeze(1).unsqueeze(0)\n",
    "        assert rlt.shape == (1, 4, 84, 84)\n",
    "        return rlt\n",
    "\n",
    "    def stack_4(\n",
    "        self, h: Episode[Observation, Action], idx: int\n",
    "    ) -> npt.NDArray[np.uint8]:\n",
    "\n",
    "        assert idx < 0\n",
    "        last_4_index = [-12 + idx, -8 + idx, -4 + idx, idx]\n",
    "\n",
    "        last_4: List[Observation] = []\n",
    "        for idx in last_4_index:\n",
    "            if -idx <= len(h):\n",
    "                last_4.append(np.asarray((h[idx][0])))\n",
    "\n",
    "        last_4_arr = np.asarray(last_4)\n",
    "        while last_4_arr.shape[0] < 4:\n",
    "            last_4_arr = np.insert(last_4_arr, 0, last_4[0], axis=0)\n",
    "\n",
    "        assert last_4_arr.shape == (4, 210, 160, 3)\n",
    "\n",
    "        return last_4_arr\n",
    "\n",
    "    def transform_history(\n",
    "        self, h: Episode[Observation, Action]\n",
    "    ) -> Episode[State, Action]:\n",
    "        assert len(h) == len(self.history) + 1\n",
    "\n",
    "        (_, a, r) = h[-1]\n",
    "        last_4_arr = self.stack_4(h, -1)\n",
    "        # s = torch.stack\n",
    "        s = torch.stack([self.trfm(i)\n",
    "                        for i in last_4_arr]).squeeze(1).unsqueeze(0)\n",
    "        assert s.shape == (1, 4, 84, 84)\n",
    "        self.history.append((s, a, r))\n",
    "\n",
    "        # if len(h) > l:\n",
    "        #     self.history.extend([(self.trfm(o), a, r) for (o, a, r) in h[l:]])\n",
    "        #     return self.history\n",
    "        # else:\n",
    "        return self.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd632695",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on shrinked mode: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [23:49<00:00, 69.93it/s] \n"
     ]
    }
   ],
   "source": [
    "# TRAINING_TIMES = 50_00_0000 / 20\n",
    "TRAINING_TIMES = 100000\n",
    "\n",
    "agent = Agent(env, NNAlgorithm(DQN(), 10_0000), Preprocess())\n",
    "\n",
    "with tqdm(total=TRAINING_TIMES) as pbar:\n",
    "    pbar.update(1)\n",
    "    frames = 1\n",
    "    while frames < TRAINING_TIMES:\n",
    "        agent.reset(['preprocess'])\n",
    "\n",
    "        end = False\n",
    "\n",
    "        while not end and frames < TRAINING_TIMES:\n",
    "            (o, end, episode) = agent.step()\n",
    "            # if frames > TRAINING_TIMES / 5 * 2:\n",
    "            #     agent.render('human')\n",
    "            frames += 1\n",
    "            pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd2983d6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:03<00:00,  6.10s/it]\n"
     ]
    }
   ],
   "source": [
    "EVALUATION_TIMES = 30\n",
    "MAX_EPISODE_LENGTH = 18_000\n",
    "rwds: List[int] = []\n",
    "agent.toggleImprove(False)\n",
    "\n",
    "for _ in tqdm(range(EVALUATION_TIMES)):\n",
    "    agent.reset(['preprocess'])\n",
    "\n",
    "    end = False\n",
    "    i = 1\n",
    "\n",
    "    while not end and i < MAX_EPISODE_LENGTH:\n",
    "        (o, end, episode) = agent.step()\n",
    "        i += 1\n",
    "        # env.render()\n",
    "        # if end:\n",
    "        #     rwds.append(np.sum([r if r is not None else 0 for (_,\n",
    "        #                                                        _, r) in cast(Episode, episode)]))\n",
    "    rwds.append(\n",
    "        np.sum([r if r is not None else 0 for (_, _, r) in agent.episode]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9444528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-21.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rwds)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
