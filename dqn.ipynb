{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804dd343",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import gym\n",
    "\n",
    "from typing import List, Tuple, Literal, Any, Optional, cast, Callable, Union, Iterable\n",
    "import plotly.graph_objects as go\n",
    "from gym.spaces import Box\n",
    "from utils.agent import Agent\n",
    "from torchvision import transforms as T\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.algorithm import AlgorithmInterface\n",
    "from utils.preprocess import PreprocessInterface\n",
    "import torch\n",
    "from gym.wrappers import FrameStack\n",
    "from collections import deque\n",
    "from torchvision import transforms\n",
    "import math\n",
    "from torch import nn\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from utils.common import Step, Episode, TransitionGeneric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba56515b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8bd056f190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d494e7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f93d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env.seed(RANDOM_SEED)\n",
    "env.reset()\n",
    "TOTAL_ACTIONS = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c43d75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ce4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env, skip: int):\n",
    "        assert skip >= 0\n",
    "\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.env = env\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        obs = None\n",
    "        info = None\n",
    "\n",
    "        for _ in range(self._skip + 1):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.obs_shape = env.observation_space.shape[:2]\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=255, shape=(1,) + self.obs_shape, dtype=np.uint8)\n",
    "\n",
    "        self.transform = T.Grayscale()\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        observation = self.transform(observation)\n",
    "        assert observation.shape == self.observation_space.shape\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env: gym.Env, _shape: Union[int, Tuple[int, int]]):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        if isinstance(_shape, int):\n",
    "            shape = (_shape, _shape)\n",
    "        else:\n",
    "            shape = _shape\n",
    "\n",
    "        self.obs_shape = self.observation_space.shape[0:1] + shape\n",
    "\n",
    "        # obs_low = self.observation_space.low\n",
    "        # obs_high = self.observation_space.high\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=255, shape=self.obs_shape, dtype=np.uint8)\n",
    "\n",
    "        self.transforms = T.Compose(\n",
    "            [T.Resize(shape)]\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.transforms(observation)\n",
    "        assert observation.shape == self.observation_space.shape\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3835d92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FrameStack<ResizeObservation<GrayScaleObservation<SkipFrame<TimeLimit<AtariEnv<PongNoFrameskip-v4>>>>>>>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, 84)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7746b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape is (210, 160, 3)\n",
    "Observation = torch.Tensor\n",
    "Action = int\n",
    "\n",
    "# shape is (4, 210, 160, 3)\n",
    "State = torch.Tensor\n",
    "Reward = int\n",
    "\n",
    "Transition = TransitionGeneric[State, Action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3e29eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, (8, 8), 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (4, 4), 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, (3, 3), 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.Linear(512, TOTAL_ACTIONS),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: State) -> torch.Tensor:\n",
    "        rlt = cast(torch.Tensor, self.net(x.to(device)))\n",
    "        assert rlt.shape == (x.shape[0], TOTAL_ACTIONS)\n",
    "        return rlt.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "415e8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAlgorithm(AlgorithmInterface[State, Action]):\n",
    "    def __init__(self):\n",
    "        # self.after_step_freq = 1\n",
    "        # self.need_on_termination = True\n",
    "        self.frame_skip = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.times = 1\n",
    "        self.last_action = None\n",
    "\n",
    "    def allowed_actions(self, state: State) -> List[Action]:\n",
    "        return list(range(TOTAL_ACTIONS))\n",
    "\n",
    "    def take_action(self, state: State) -> Action:\n",
    "        self.times += 1\n",
    "\n",
    "        if self.times % 10 == 0:\n",
    "            act = np.random.choice(self.allowed_actions(state))\n",
    "            self.last_action = act\n",
    "            return act\n",
    "\n",
    "        if self.last_action is not None:\n",
    "            return self.last_action\n",
    "\n",
    "        act = np.random.choice(self.allowed_actions(state))\n",
    "        self.last_action = act\n",
    "        return act\n",
    "\n",
    "    def after_step(\n",
    "        self,\n",
    "        sar: Tuple[State, Action, Reward],\n",
    "        sa: Tuple[State, Optional[Action]],\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a94ee723",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TRAINING_TIMES = 50_00_0000\n",
    "\n",
    "\n",
    "class NNAlgorithm(AlgorithmInterface[State, Action]):\n",
    "    def __init__(self, training_times: int = 50_00_0000, gamma: float = 0.99):\n",
    "        self.frame_skip = 0\n",
    "\n",
    "        self.times = 0\n",
    "\n",
    "        self.policy_network = DQN().to(device)\n",
    "        self.optimizer = torch.optim.RMSprop(\n",
    "            self.policy_network.parameters(), 1e-3)\n",
    "\n",
    "        self.shrink = min(training_times / DEFAULT_TRAINING_TIMES, 1)\n",
    "        if self.shrink != 1:\n",
    "            print(f\"training on shrinked mode: {self.shrink}\")\n",
    "\n",
    "        self.target_network = DQN().to(device)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        for p in self.target_network.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.update_target = 1000\n",
    "\n",
    "        self.memory_replay: deque[Transition] = deque(\n",
    "            maxlen=math.ceil(25_0000 * self.shrink)\n",
    "        )\n",
    "        self.gamma = gamma\n",
    "        self.loss_func = torch.nn.MSELoss().to(device)\n",
    "\n",
    "        self.loss: List[float] = []\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def allowed_actions(self, _: State) -> List[Action]:\n",
    "        return list(range(TOTAL_ACTIONS))\n",
    "\n",
    "    def take_action(self, state: State) -> Action:\n",
    "        rand = np.random.random()\n",
    "        max_decry_times = 100_0000 * self.shrink\n",
    "        sigma = 1 - 0.9 / max_decry_times * \\\n",
    "            np.min([self.times, max_decry_times])\n",
    "        if rand < sigma:\n",
    "            return np.random.choice(self.allowed_actions(state))\n",
    "\n",
    "        else:\n",
    "            act_vals: torch.Tensor = self.policy_network(\n",
    "                self.resolve_lazy_frames(state)\n",
    "            )\n",
    "            maxi = torch.argmax(act_vals)\n",
    "            return cast(int, maxi.item())\n",
    "\n",
    "    def after_step(\n",
    "        self,\n",
    "        sar: Tuple[State, Action, Reward],\n",
    "        sa: Tuple[State, Optional[Action]],\n",
    "    ):\n",
    "        (s, a, r) = sar\n",
    "        (sn, an) = sa\n",
    "        self.memory_replay.append((s, a, r, sn, an))\n",
    "\n",
    "        if len(self.memory_replay) >= 1.25 * self.batch_size:\n",
    "\n",
    "            batch: List[Transition] = []\n",
    "            for i in np.random.choice(len(self.memory_replay), self.batch_size):\n",
    "                batch.append(self.memory_replay[i])\n",
    "\n",
    "            self.train(batch)\n",
    "\n",
    "        if self.times != 0 and self.times % (self.update_target) == 0:\n",
    "            self.update_target_network()\n",
    "\n",
    "        self.times += 1\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "    def clip_reward(self, r: float) -> float:\n",
    "        if r > 0:\n",
    "            return 1.0\n",
    "        elif r < 0:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def resolve_lazy_frames(self, s: State) -> torch.Tensor:\n",
    "        return torch.cat([s[0], s[1], s[2], s[3]]).unsqueeze(0)\n",
    "\n",
    "    def train(self, batch: List[Transition]):\n",
    "\n",
    "        masks = torch.tensor(\n",
    "            [0 if an is None else 1 for (_, _, _, _, an) in batch],\n",
    "            dtype=torch.float,\n",
    "        )\n",
    "\n",
    "        # target = torch.tensor(\n",
    "        #     [self.clip_reward(r) for (_, _, r, _, _) in batch], dtype=torch.float\n",
    "        # ) + torch.inner(\n",
    "        #     masks,\n",
    "        #     self.gamma\n",
    "        #     * torch.max(\n",
    "        #         self.target_network(\n",
    "        #             torch.cat([self.resolve_lazy_frames(sn)\n",
    "        #                       for (_, _, _, sn, _) in batch])),\n",
    "        #         dim=1,\n",
    "        #     )[0],\n",
    "        # )\n",
    "        s_next = torch.cat([self.resolve_lazy_frames(sn)\n",
    "                            for (_, _, _, sn, _) in batch])\n",
    "        assert s_next.shape == (32, 4, 84, 84)\n",
    "        q_next = self.target_network(s_next)\n",
    "\n",
    "        assert q_next.shape == (32, TOTAL_ACTIONS)\n",
    "\n",
    "        target = torch.tensor(\n",
    "            [self.clip_reward(r) for (_, _, r, _, _) in batch], dtype=torch.float\n",
    "        ) + torch.inner(\n",
    "            masks,\n",
    "            self.gamma\n",
    "            * q_next.gather(1, torch.argmax(self.policy_network(s_next), dim=1, keepdim=True)).squeeze(1)\n",
    "        )\n",
    "\n",
    "        assert target.shape == (32,)\n",
    "        s_curr = torch.cat([self.resolve_lazy_frames(s)\n",
    "                            for (s, _, _, _, _) in batch])\n",
    "        assert s_curr.shape == (32, 4, 84, 84)\n",
    "\n",
    "        x_vals = self.policy_network(s_curr)\n",
    "\n",
    "        x = x_vals.gather(1, torch.tensor(\n",
    "            [a for (_, a, _, _, _) in batch]).unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        assert x.shape == (32,)\n",
    "\n",
    "        loss = self.loss_func(x, target)\n",
    "        self.loss.append(loss.item())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def on_termination(self, sar: Tuple[List[State], List[Action], List[Reward]]):\n",
    "        (s, a, r) = sar\n",
    "        assert len(s) == len(a) + 1\n",
    "        assert len(s) == len(r) + 1\n",
    "        pass\n",
    "\n",
    "\n",
    "class Preprocess(PreprocessInterface[Observation, Action, State]):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def get_current_state(self, h: List[Observation]) -> State:\n",
    "        assert len(h) > 0\n",
    "\n",
    "        assert h[-1].shape == (4, 1, 84, 84)\n",
    "        return h[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd632695",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14957/50000000 [02:22<132:17:55, 104.95it/s, loss=1.76e+12, memory_ratio=0.299, rwd=-20, times=0.015]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/diyuan/openai-gym/dqn.ipynb Cell 12'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000011?line=18'>19</a>\u001b[0m \u001b[39m# while frames < TRAINING_TIMES:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000011?line=19'>20</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m end \u001b[39mand\u001b[39;00m frames \u001b[39m<\u001b[39m TRAINING_TIMES:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000011?line=20'>21</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000011?line=21'>22</a>\u001b[0m     \u001b[39m# end = False\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000011?line=22'>23</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000011?line=23'>24</a>\u001b[0m     \u001b[39m# while not end and frames < TRAINING_TIMES:\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000011?line=24'>25</a>\u001b[0m     (_, end) \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000011?line=25'>26</a>\u001b[0m     \u001b[39m# pbar.update(1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000011?line=26'>27</a>\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/openai-gym/utils/agent.py:79\u001b[0m, in \u001b[0;36mAgent.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/diyuan/openai-gym/utils/agent.py?line=75'>76</a>\u001b[0m stop: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/diyuan/openai-gym/utils/agent.py?line=77'>78</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgm\u001b[39m.\u001b[39mframe_skip \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='file:///home/diyuan/openai-gym/utils/agent.py?line=78'>79</a>\u001b[0m     (o, r, s, _) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(act)\n\u001b[1;32m     <a href='file:///home/diyuan/openai-gym/utils/agent.py?line=79'>80</a>\u001b[0m     rwd \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r\n\u001b[1;32m     <a href='file:///home/diyuan/openai-gym/utils/agent.py?line=80'>81</a>\u001b[0m     obs \u001b[39m=\u001b[39m o\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gym/wrappers/frame_stack.py:115\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    <a href='file:///home/diyuan/.local/lib/python3.9/site-packages/gym/wrappers/frame_stack.py?line=113'>114</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> <a href='file:///home/diyuan/.local/lib/python3.9/site-packages/gym/wrappers/frame_stack.py?line=114'>115</a>\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    <a href='file:///home/diyuan/.local/lib/python3.9/site-packages/gym/wrappers/frame_stack.py?line=115'>116</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframes\u001b[39m.\u001b[39mappend(observation)\n\u001b[1;32m    <a href='file:///home/diyuan/.local/lib/python3.9/site-packages/gym/wrappers/frame_stack.py?line=116'>117</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(), reward, done, info\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gym/core.py:323\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    <a href='file:///home/diyuan/.local/lib/python3.9/site-packages/gym/core.py?line=321'>322</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> <a href='file:///home/diyuan/.local/lib/python3.9/site-packages/gym/core.py?line=322'>323</a>\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    <a href='file:///home/diyuan/.local/lib/python3.9/site-packages/gym/core.py?line=323'>324</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gym/core.py:324\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    <a href='file:///home/diyuan/.local/lib/python3.9/site-packages/gym/core.py?line=321'>322</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    <a href='file:///home/diyuan/.local/lib/python3.9/site-packages/gym/core.py?line=322'>323</a>\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m--> <a href='file:///home/diyuan/.local/lib/python3.9/site-packages/gym/core.py?line=323'>324</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation(observation), reward, done, info\n",
      "\u001b[1;32m/home/diyuan/openai-gym/dqn.ipynb Cell 6'\u001b[0m in \u001b[0;36mGrayScaleObservation.observation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000005?line=42'>43</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobservation\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000005?line=43'>44</a>\u001b[0m     observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpermute_orientation(observation)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000005?line=44'>45</a>\u001b[0m     observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(observation)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000005?line=45'>46</a>\u001b[0m     \u001b[39massert\u001b[39;00m observation\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;32m/home/diyuan/openai-gym/dqn.ipynb Cell 6'\u001b[0m in \u001b[0;36mGrayScaleObservation.permute_orientation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000005?line=36'>37</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpermute_orientation\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000005?line=37'>38</a>\u001b[0m     \u001b[39m# permute [H, W, C] array to [C, H, W] tensor\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000005?line=38'>39</a>\u001b[0m     observation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(observation, (\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000005?line=39'>40</a>\u001b[0m     observation \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(observation\u001b[39m.\u001b[39;49mcopy(), dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/dqn.ipynb#ch0000005?line=40'>41</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m observation\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TRAINING_TIMES = DEFAULT_TRAINING_TIMES\n",
    "# TRAINING_TIMES = 2_0000\n",
    "# env._max_episode_steps = 1_000\n",
    "\n",
    "agent = Agent(env, NNAlgorithm(TRAINING_TIMES), Preprocess())\n",
    "training_rwds: List[int] = []\n",
    "\n",
    "with tqdm(total=DEFAULT_TRAINING_TIMES) as pbar:\n",
    "    # for _ in pbar:\n",
    "    frames = 0\n",
    "    # pbar.update(1)\n",
    "    # pbar.update(1)\n",
    "    # frames = 1\n",
    "    while frames < TRAINING_TIMES:\n",
    "        agent.reset([\"preprocess\"])\n",
    "        # frames += 1\n",
    "        i = 0\n",
    "        end = False\n",
    "        # while frames < TRAINING_TIMES:\n",
    "        while not end and frames < TRAINING_TIMES:\n",
    "\n",
    "            # end = False\n",
    "\n",
    "            # while not end and frames < TRAINING_TIMES:\n",
    "            (_, end) = agent.step()\n",
    "            # pbar.update(1)\n",
    "            i += 1\n",
    "\n",
    "            # frames += 1\n",
    "            # pbar.update(1)\n",
    "        pbar.update(i)\n",
    "\n",
    "        training_rwds.append(np.sum([r for r in agent.episode_reward]))\n",
    "        pbar.set_postfix(\n",
    "            rwd=training_rwds[-1],\n",
    "            times=min(agent.algm.times / 100_0000, 1),\n",
    "            memory_ratio=len(agent.algm.memory_replay) / 5_0000,\n",
    "            loss=agent.algm.loss[-1]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2264d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.algm.times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ea829",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./training.arr\", np.asarray(training_rwds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84085436",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[i + 1 for i in range(len(training_rwds))],\n",
    "               y = [r for r in training_rwds])\n",
    ")\n",
    "# fig.update_yaxes(type=\"log\")\n",
    "# fig.update_layout(yaxis_type=\"log\")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0befcf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[i + 1 for i in range(len(agent.algm.loss))],\n",
    "               y = [r for r in agent.algm.loss])\n",
    ")\n",
    "# fig.update_yaxes(type=\"log\")\n",
    "# fig.update_layout(yaxis_type=\"log\")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2983d6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "EVALUATION_TIMES = 30\n",
    "MAX_EPISODE_LENGTH = 18_000\n",
    "rwds: List[int] = []\n",
    "agent.toggleEval(True)\n",
    "\n",
    "for _ in tqdm(range(EVALUATION_TIMES)):\n",
    "    agent.reset(['preprocess'])\n",
    "\n",
    "    end = False\n",
    "    i = 1\n",
    "\n",
    "    while not end and i < MAX_EPISODE_LENGTH:\n",
    "        (o, end) = agent.step()\n",
    "        i += 1\n",
    "        env.render()\n",
    "        # if end:\n",
    "        #     rwds.append(np.sum([r if r is not None else 0 for (_,\n",
    "        #                                                        _, r) in cast(Episode, episode)]))\n",
    "    rwds.append(\n",
    "        np.sum([r for r in agent.episode_reward])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b79e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./eval.arr\", np.asarray(rwds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9444528",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[i + 1 for i in range(len(rwds))],\n",
    "               y = [r for r in rwds])\n",
    ")\n",
    "# fig.update_yaxes(type=\"log\")\n",
    "# fig.update_layout(yaxis_type=\"log\")\n",
    "fig.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
