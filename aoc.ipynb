{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from gym.envs.toy_text import BlackjackEnv\n",
    "import gym\n",
    "from typing import Literal, List, Tuple, cast, Dict, Optional, Callable, Protocol, Union\n",
    "import plotly.graph_objects as go\n",
    "from copy import deepcopy\n",
    "from abc import abstractmethod, ABC\n",
    "import math\n",
    "import sys\n",
    "import plotly.express as px\n",
    "\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<MountainCarEnv<MountainCar-v0>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(RANDOM_SEED)\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.589128,  0.      ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 19:23:53.330 Python[19064:269017] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/nv/xnzxl_4n4fq2qgjf6b4vh5p00000gp/T/com.apple.python3.savedState\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Position = float\n",
    "Velocity = float\n",
    "State = Tuple[Position, Velocity]\n",
    "\n",
    "Action = Literal[0, 1, 2]\n",
    "\n",
    "StateAction = Tuple[State, Action]\n",
    "Observation = State\n",
    "\n",
    "\"\"\"\n",
    "- 0: Accelerate to the Left\n",
    "- 1: Don't accelerate\n",
    "- 2: Accelerate to the Right\n",
    "\"\"\"\n",
    "Reward = float\n",
    "Step = Tuple[State, Optional[Action], Optional[Reward]]\n",
    "Episode = List[Step]\n",
    "\n",
    "# all_states: List[State] = list(range(1000))\n",
    "all_actions: List[Action] = [0, 1, 2]\n",
    "# nums_of_all_state = len(all_states)\n",
    "# nums_of_all_state_action = len(all_states) * len(all_actions)\n",
    "# allowed_actions: List[List[Action]] = [\n",
    "#     all_actions for _ in range(nums_of_all_state)]\n",
    "\n",
    "\n",
    "Feature = np.ndarray\n",
    "Weight = np.ndarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureInterface(Protocol):\n",
    "    len: int\n",
    "\n",
    "    @abstractmethod\n",
    "    def to_feature(self, sa: StateAction) -> Feature:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class TestFeature(FeatureInterface):\n",
    "    def __init__(self):\n",
    "        self.len = 5\n",
    "\n",
    "    def to_feature(self, sa: StateAction) -> Feature:\n",
    "        ((pos, vel), a) = sa\n",
    "        v = [pos, vel, *self.one_hot_encode(a)]\n",
    "        assert len(v) == 5, f\"unexpected length encountered: {len(v)}\"\n",
    "        return np.asarray(v)\n",
    "\n",
    "    def one_hot_encode(self, a: Action) -> List[int]:\n",
    "        assert a in [0, 1, 2], f\"unexpected a encountered: {a}\"\n",
    "        if a == 0:\n",
    "            return [1, 0, 0]\n",
    "        elif a == 1:\n",
    "            return [0, 1, 0]\n",
    "        else:\n",
    "            return [0, 0, 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AppxInterface(Protocol):\n",
    "    feature_algorithm: FeatureInterface\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, sa: StateAction, w: Weight) -> float:\n",
    "        raise NotImplemented()\n",
    "\n",
    "    @abstractmethod\n",
    "    def gradient(self, sa: StateAction, w: Weight) -> np.ndarray:\n",
    "        raise NotImplemented()\n",
    "\n",
    "\n",
    "class Linear(AppxInterface):\n",
    "    def __init__(self, feature_algorithm: FeatureInterface):\n",
    "        self.feature_algorithm = feature_algorithm\n",
    "\n",
    "    def predict(self, sa: StateAction, w: Weight) -> float:\n",
    "        return np.inner(self.feature_algorithm.to_feature(sa), w)\n",
    "\n",
    "    def gradient(self, sa: StateAction, w: Weight) -> np.ndarray:\n",
    "        return self.feature_algorithm.to_feature(sa)\n",
    "\n",
    "\n",
    "class PolicyInterface(Protocol):\n",
    "    appx_algorithm: AppxInterface\n",
    "\n",
    "    @abstractmethod\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def take_action(self, s: State, w: Weight) -> Action:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class SigmaGreddy(PolicyInterface):\n",
    "    def __init__(self, sigma: float, appx_algorithm: AppxInterface):\n",
    "        self.sigma = sigma\n",
    "        self.appx_algorithm = appx_algorithm\n",
    "\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        return all_actions\n",
    "\n",
    "    def take_action(self, s: State, w: Weight) -> Action:\n",
    "        rand = np.random.random()\n",
    "        all_actions = self.allowed_actions(s)\n",
    "        if rand < self.sigma:\n",
    "            return np.random.choice(all_actions)\n",
    "        else:\n",
    "            maxi = np.argmax(\n",
    "                [self.appx_algorithm.predict((s, a), w) for a in all_actions]\n",
    "            )\n",
    "            return all_actions[maxi]\n",
    "\n",
    "class AlwaysRight(PolicyInterface):\n",
    "\n",
    "    def __init__(self, appx_algorithm: AppxInterface):\n",
    "        self.appx_algorithm = appx_algorithm\n",
    "\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        return all_actions\n",
    "    def take_action(self, s: State, w: Weight) -> Action:\n",
    "        return 2\n",
    "\n",
    "\n",
    "class AlgorithmInterface(Protocol):\n",
    "    n_of_omega: int\n",
    "    policy_algorithm: PolicyInterface\n",
    "\n",
    "    @abstractmethod\n",
    "    def after_step(\n",
    "        self, cur_state_action: StateAction, episode: Episode, omega: np.ndarray\n",
    "    ):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_termination(self, episode: Episode, omega: np.ndarray):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        return self.policy_algorithm.allowed_actions(s)\n",
    "\n",
    "    def is_terminal_state(self, s: State) -> bool:\n",
    "        (pos, _) = s\n",
    "        if pos >= 0.5:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def take_action(self, s: State, omega: Weight) -> Action:\n",
    "        if self.is_terminal_state(s):\n",
    "            return np.random.choice(self.allowed_actions(s))\n",
    "\n",
    "        return self.policy_algorithm.take_action(s, omega)\n",
    "\n",
    "    def predict(self, sa: StateAction, w: Weight) -> float:\n",
    "        # assert -1 <= s <= len(all_states), f\"unexpected state encounter: {s}\"\n",
    "        (s, _) = sa\n",
    "        if self.is_terminal_state(s):\n",
    "            return 0\n",
    "\n",
    "        # if s == -1 or s == len(all_states):\n",
    "        #     return 0\n",
    "\n",
    "        return self.policy_algorithm.appx_algorithm.predict(sa, w)\n",
    "\n",
    "    def gradient(self, sa: StateAction, w: Weight) -> np.ndarray:\n",
    "        # assert 0 <= s < len(all_states), f\"unexpected state encounter: {s}\"\n",
    "\n",
    "        return self.policy_algorithm.appx_algorithm.gradient(sa, w)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Sarsa(AlgorithmInterface):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float,\n",
    "        policy_algorithm=PolicyInterface,\n",
    "        gamma: float = 1.0,\n",
    "    ):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.policy_algorithm = policy_algorithm\n",
    "\n",
    "        self.n_of_omega = self.policy_algorithm.appx_algorithm.feature_algorithm.len\n",
    "\n",
    "    def after_step(\n",
    "        self, this_state_action: StateAction, episode: Episode, omega: np.ndarray\n",
    "    ):\n",
    "        # (this_s, this_a) = this_state_action\n",
    "        history = episode[-1:]\n",
    "        gamma = self.gamma\n",
    "\n",
    "        # if len(history) != (1 + n):\n",
    "        #     return\n",
    "        assert len(history) == (\n",
    "            1\n",
    "        ), f\"unexpected history length encountered: {len(history)}\"\n",
    "\n",
    "        (old_s, old_a, r) = history[0]\n",
    "\n",
    "        rwd = cast(Reward, r) + gamma * self.predict(this_state_action, omega)\n",
    "\n",
    "        omega += (\n",
    "            self.alpha\n",
    "            * (rwd - self.predict((old_s, cast(Action, old_a)), omega))\n",
    "            * self.gradient((old_s, cast(Action, old_a)), omega)\n",
    "        )\n",
    "\n",
    "    def on_termination(self, episode: Episode, omega: Weight):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        algm: AlgorithmInterface,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.algm = algm\n",
    "        self.clear()\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_state: State = self.env.reset()\n",
    "        self.ready_act: Optional[Action] = None\n",
    "        self.end = False\n",
    "        self.episode: Episode = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.reset()\n",
    "\n",
    "        self.omega = np.asarray(\n",
    "            [np.random.random() for _ in range(self.algm.n_of_omega)]\n",
    "        )\n",
    "        self.episodes: List[Episode] = []\n",
    "\n",
    "    def step(self) -> Tuple[Observation, bool]:\n",
    "        assert not self.end, \"cannot step on a ended agent\"\n",
    "\n",
    "        act = self.ready_act or self.algm.take_action(self.cur_state, self.omega)\n",
    "        (obs, rwd, stop, _) = self.env.step(act)\n",
    "        obs = cast(Observation, obs)\n",
    "\n",
    "        self.episode.append((self.cur_state, act, rwd))\n",
    "\n",
    "        self.cur_state = obs\n",
    "\n",
    "        self.ready_act = self.algm.take_action(self.cur_state, self.omega)\n",
    "\n",
    "        self.algm.after_step((self.cur_state, self.ready_act), self.episode, self.omega)\n",
    "\n",
    "        if stop:\n",
    "            self.episode.append((self.cur_state, None, None))\n",
    "            self.episodes.append(self.episode)\n",
    "            self.episode = []\n",
    "            self.end = True\n",
    "\n",
    "            self.algm.on_termination(self.episodes[-1], self.omega)\n",
    "\n",
    "        return (obs, stop)\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "    \n",
    "    def close(self):\n",
    "        self.clear()\n",
    "        self.env.close()\n",
    "\n",
    "    def predict(self, s: State) -> float:\n",
    "        return np.max(\n",
    "            [\n",
    "                self.algm.predict((s, a), self.omega)\n",
    "                for a in self.algm.allowed_actions(s)\n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52, 12,  1,  0,  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tF = TestFeature()\n",
    "tF.to_feature(((52, 12), 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_EPISODES = 1_0000\n",
    "\n",
    "agent = Agent(\n",
    "    cast(gym.Env, env),\n",
    "    # Sarsa(2e-2, SigmaGreddy(0.1, Linear(TestFeature())))\n",
    "    Sarsa(2e-2, SigmaGreddy(0.1, Linear(TestFeature())))\n",
    "    # TDN(9, 2e-4, Linear(), Tiling(5))\n",
    ")\n",
    "\n",
    "for _ in range(TOTAL_EPISODES):\n",
    "    agent.reset()\n",
    "    end = False\n",
    "    i = 0\n",
    "    while not end:\n",
    "        _, end = agent.step()\n",
    "        agent.render()\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "agent.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.episodes[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = np.load(\"./true_values_arr.npy\", allow_pickle=False)\n",
    "true_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure()\n",
    "# s = list(range(1000))\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(x=[i + 1 for i in s], y=true_values, mode=\"lines\", name=\"true values\")\n",
    "# )\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[i + 1 for i in s],\n",
    "#         y=[agent.predict(i) for i in s],\n",
    "#         mode=\"lines\",\n",
    "#         name=\"monte-carlo prediction\",\n",
    "#     )\n",
    "# )\n",
    "# fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
