{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from gym.envs.toy_text import BlackjackEnv\n",
    "import gym\n",
    "from typing import Literal, List, Tuple, cast, Dict, Optional, Callable, Protocol, Union\n",
    "import plotly.graph_objects as go\n",
    "from copy import deepcopy\n",
    "from abc import abstractmethod, ABC\n",
    "import math\n",
    "import sys\n",
    "from primefac import primegen\n",
    "from tqdm.autonotebook import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<MountainCarEnv<MountainCar-v0>>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(RANDOM_SEED)\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.589128,  0.      ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Position = float\n",
    "Velocity = float\n",
    "State = Tuple[Position, Velocity]\n",
    "\n",
    "Action = Literal[0, 1, 2]\n",
    "\n",
    "StateAction = Tuple[State, Action]\n",
    "Observation = State\n",
    "\n",
    "\"\"\"\n",
    "- 0: Accelerate to the Left\n",
    "- 1: Don't accelerate\n",
    "- 2: Accelerate to the Right\n",
    "\"\"\"\n",
    "Reward = float\n",
    "Step = Tuple[State, Optional[Action], Optional[Reward]]\n",
    "Episode = List[Step]\n",
    "\n",
    "# all_states: List[State] = list(range(1000))\n",
    "all_actions: List[Action] = [0, 1, 2]\n",
    "# nums_of_all_state = len(all_states)\n",
    "# nums_of_all_state_action = len(all_states) * len(all_actions)\n",
    "# allowed_actions: List[List[Action]] = [\n",
    "#     all_actions for _ in range(nums_of_all_state)]\n",
    "\n",
    "\n",
    "Feature = np.ndarray\n",
    "Weight = np.ndarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureInterface(Protocol):\n",
    "    len: int\n",
    "\n",
    "    @abstractmethod\n",
    "    def to_feature(self, sa: StateAction) -> Feature:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def one_hot_encode(self, a: Action) -> List[int]:\n",
    "        assert a in all_actions, f\"bad action encountered: {a}\"\n",
    "        if a == 0:\n",
    "            return [1, 0, 0]\n",
    "        elif a == 1:\n",
    "            return [0, 1, 0]\n",
    "        else:\n",
    "            return [0, 0, 1]\n",
    "\n",
    "\n",
    "class Tiling(FeatureInterface):\n",
    "    def __init__(self, n_tilings: int, input_range: List[Tuple[float, float]]):\n",
    "\n",
    "        if n_tilings // 2 == 0:\n",
    "            n_tilings = n_tilings + 1\n",
    "\n",
    "        assert n_tilings > 1, f\"number of tilings cannot be lower than 2: {n_tilings}\"\n",
    "\n",
    "        self.input_range = input_range\n",
    "        self.dimension = len(input_range)\n",
    "\n",
    "        assert self.dimension >= 1, f\"cannot manupilate on 0-dimension\"\n",
    "\n",
    "        self.n_tilings = n_tilings\n",
    "        # self.n_tiles_per_tiling = n_tilings\n",
    "        # self.len = n_tilings * (2 * n_tilings - 1)\n",
    "        self.len = n_tilings * n_tilings * self.dimension + 3\n",
    "\n",
    "        self.tilings = self.compute_tilings()\n",
    "\n",
    "    def get_tiling_point(self, nums: List[float], n_partitions: int) -> List[float]:\n",
    "        assert len(nums) >= 2, f\"unexpected nums encountered: {nums}\"\n",
    "        assert n_partitions >= 2, f\"unexpected n_partitions encountered: {n_partitions}\"\n",
    "\n",
    "        (l, r) = (nums[0], nums[-1])\n",
    "\n",
    "        delta = (r - l) / n_partitions\n",
    "\n",
    "        return [l, *[l + i * delta for i in range(1, n_partitions)], r]\n",
    "\n",
    "    def find_lowest_prime(self) -> int:\n",
    "        return list(primegen(100 / self.n_tilings))[-1]\n",
    "\n",
    "    def compute_tilings(self) -> List[List[List[float]]]:\n",
    "        points: List[List[List[float]]] = []\n",
    "\n",
    "        for (l, r) in self.input_range:\n",
    "            points_one_dimen: List[List[float]] = []\n",
    "\n",
    "            pivot_points = self.get_tiling_point([l, r], self.n_tilings)\n",
    "            points_one_dimen.append(pivot_points)\n",
    "\n",
    "            move_delta = self.find_lowest_prime()\n",
    "\n",
    "            # print(int(self.n_tilings + 1) / 2)\n",
    "            for i in range(1, int((self.n_tilings + 1) / 2)):\n",
    "                points_one_dimen.append(\n",
    "                    self.move_tiling_points(pivot_points, move_delta * i)\n",
    "                )\n",
    "                points_one_dimen.append(\n",
    "                    self.move_tiling_points(pivot_points, -1 * move_delta * i)\n",
    "                )\n",
    "\n",
    "            points.append(points_one_dimen)\n",
    "\n",
    "        return points\n",
    "\n",
    "    def point_in_range(self, p: float, rang: List[float]) -> List[Literal[0, 1]]:\n",
    "        v: List[Literal[0, 1]] = []\n",
    "        for i in range(1, len(rang)):\n",
    "            if rang[i - 1] <= p < rang[i]:\n",
    "                v.append(1)\n",
    "            else:\n",
    "                v.append(0)\n",
    "\n",
    "        assert (\n",
    "            len(v) == len(rang) - 1\n",
    "        ), f\"bad length of v encountered: {len(v)}, {len(rang)-1}\"\n",
    "\n",
    "        return v\n",
    "\n",
    "    def to_feature(self, sa: StateAction) -> Feature:\n",
    "        ((pos, vel), a) = sa\n",
    "        pos_dim = self.tilings[0]\n",
    "        vel_dim = self.tilings[1]\n",
    "\n",
    "        pos_feat = [self.point_in_range(pos, t) for t in pos_dim]\n",
    "        vel_feat = [self.point_in_range(vel, t) for t in vel_dim]\n",
    "\n",
    "        pos_feat_one_dim = [inner for outer in pos_feat for inner in outer]\n",
    "        vel_feat_one_dim = [inner for outer in vel_feat for inner in outer]\n",
    "\n",
    "        return np.asarray(\n",
    "            [*pos_feat_one_dim, *vel_feat_one_dim, *self.one_hot_encode(a)]\n",
    "        )\n",
    "\n",
    "    def move_tiling_points(\n",
    "        self,\n",
    "        points: List[float],\n",
    "        percentile: int,\n",
    "    ) -> List[float]:\n",
    "        # return [(np.round(lp + delta), np.round(rp + delta)) for (lp, rp) in tiling]\n",
    "        # delta = splitting_points[1] - splitting_points[0]\n",
    "        assert len(points) >= 2, f\"bad points: {points}\"\n",
    "\n",
    "        rang = points[-1] - points[0]\n",
    "\n",
    "        return [p + percentile / 100 * rang for p in points]\n",
    "\n",
    "\n",
    "class TestFeature(FeatureInterface):\n",
    "    def __init__(self):\n",
    "        self.len = 8\n",
    "\n",
    "    def to_feature(self, sa: StateAction) -> Feature:\n",
    "        ((pos, vel), a) = sa\n",
    "        v = [1, pos, vel, pos + vel, pos * vel, *self.one_hot_encode(a)]\n",
    "        assert len(v) == self.len, f\"unexpected length encountered: {len(v)}\"\n",
    "        return np.asarray(v)\n",
    "\n",
    "\n",
    "class AppxInterface(Protocol):\n",
    "    feature_algorithm: FeatureInterface\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, sa: StateAction, w: Weight) -> float:\n",
    "        raise NotImplemented()\n",
    "\n",
    "    @abstractmethod\n",
    "    def gradient(self, sa: StateAction, w: Weight) -> np.ndarray:\n",
    "        raise NotImplemented()\n",
    "\n",
    "\n",
    "class Linear(AppxInterface):\n",
    "    def __init__(self, feature_algorithm: FeatureInterface):\n",
    "        self.feature_algorithm = feature_algorithm\n",
    "\n",
    "    def predict(self, sa: StateAction, w: Weight) -> float:\n",
    "        return np.inner(self.feature_algorithm.to_feature(sa), w)\n",
    "\n",
    "    def gradient(self, sa: StateAction, w: Weight) -> np.ndarray:\n",
    "        return self.feature_algorithm.to_feature(sa)\n",
    "\n",
    "\n",
    "class PolicyInterface(Protocol):\n",
    "    appx_algorithm: AppxInterface\n",
    "\n",
    "    @abstractmethod\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def take_action(self, s: State, w: Weight) -> Action:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class SigmaGreddy(PolicyInterface):\n",
    "    def __init__(self, sigma: float, appx_algorithm: AppxInterface):\n",
    "        self.sigma = sigma\n",
    "        self.appx_algorithm = appx_algorithm\n",
    "\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        return all_actions\n",
    "\n",
    "    def take_action(self, s: State, w: Weight) -> Action:\n",
    "        rand = np.random.random()\n",
    "        all_actions = self.allowed_actions(s)\n",
    "        if rand < self.sigma:\n",
    "            return np.random.choice(all_actions)\n",
    "        else:\n",
    "            maxi = np.argmax(\n",
    "                [self.appx_algorithm.predict((s, a), w) for a in all_actions]\n",
    "            )\n",
    "            return all_actions[maxi]\n",
    "\n",
    "\n",
    "class AlwaysRight(PolicyInterface):\n",
    "    def __init__(self, appx_algorithm: AppxInterface):\n",
    "        self.appx_algorithm = appx_algorithm\n",
    "\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        return all_actions\n",
    "\n",
    "    def take_action(self, s: State, w: Weight) -> Action:\n",
    "        return 2\n",
    "\n",
    "\n",
    "class AlgorithmInterface(Protocol):\n",
    "    n_of_omega: int\n",
    "    policy_algorithm: PolicyInterface\n",
    "\n",
    "    @abstractmethod\n",
    "    def after_step(\n",
    "        self, cur_state_action: StateAction, episode: Episode, omega: np.ndarray\n",
    "    ):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_termination(self, episode: Episode, omega: np.ndarray):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        return self.policy_algorithm.allowed_actions(s)\n",
    "\n",
    "    def is_terminal_state(self, s: State) -> bool:\n",
    "        (pos, _) = s\n",
    "        if pos >= 0.5:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def take_action(self, s: State, omega: Weight) -> Action:\n",
    "        if self.is_terminal_state(s):\n",
    "            return np.random.choice(self.allowed_actions(s))\n",
    "\n",
    "        return self.policy_algorithm.take_action(s, omega)\n",
    "\n",
    "    def predict(self, sa: StateAction, w: Weight) -> float:\n",
    "        # assert -1 <= s <= len(all_states), f\"unexpected state encounter: {s}\"\n",
    "        (s, _) = sa\n",
    "        if self.is_terminal_state(s):\n",
    "            return 0\n",
    "\n",
    "        # if s == -1 or s == len(all_states):\n",
    "        #     return 0\n",
    "\n",
    "        return self.policy_algorithm.appx_algorithm.predict(sa, w)\n",
    "\n",
    "    def gradient(self, sa: StateAction, w: Weight) -> np.ndarray:\n",
    "        # assert 0 <= s < len(all_states), f\"unexpected state encounter: {s}\"\n",
    "\n",
    "        return self.policy_algorithm.appx_algorithm.gradient(sa, w)\n",
    "\n",
    "\n",
    "class Sarsa(AlgorithmInterface):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float,\n",
    "        policy_algorithm=PolicyInterface,\n",
    "        gamma: float = 1.0,\n",
    "    ):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.policy_algorithm = policy_algorithm\n",
    "\n",
    "        self.n_of_omega = self.policy_algorithm.appx_algorithm.feature_algorithm.len\n",
    "\n",
    "    def after_step(\n",
    "        self, this_state_action: StateAction, episode: Episode, omega: np.ndarray\n",
    "    ):\n",
    "        # (this_s, this_a) = this_state_action\n",
    "        history = episode[-1:]\n",
    "        gamma = self.gamma\n",
    "\n",
    "        # if len(history) != (1 + n):\n",
    "        #     return\n",
    "        assert len(history) == (\n",
    "            1\n",
    "        ), f\"unexpected history length encountered: {len(history)}\"\n",
    "\n",
    "        (old_s, old_a, r) = history[0]\n",
    "\n",
    "        rwd = cast(Reward, r) + gamma * self.predict(this_state_action, omega)\n",
    "\n",
    "        omega += (\n",
    "            self.alpha\n",
    "            * (rwd - self.predict((old_s, cast(Action, old_a)), omega))\n",
    "            * self.gradient((old_s, cast(Action, old_a)), omega)\n",
    "        )\n",
    "\n",
    "    def on_termination(self, episode: Episode, omega: Weight):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        algm: AlgorithmInterface,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.algm = algm\n",
    "        self.clear()\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_state: State = self.env.reset()\n",
    "        self.ready_act: Optional[Action] = None\n",
    "        self.end = False\n",
    "        self.episode: Episode = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.reset()\n",
    "\n",
    "        self.omega = np.asarray(\n",
    "            # [np.random.random() for _ in range(self.algm.n_of_omega)]\n",
    "            [0.0 for _ in range(self.algm.n_of_omega)]\n",
    "        )\n",
    "        # self.episodes: List[Episode] = []\n",
    "\n",
    "    def step(self) -> Tuple[Observation, bool, Optional[Episode]]:\n",
    "        assert not self.end, \"cannot step on a ended agent\"\n",
    "\n",
    "        act = self.ready_act or self.algm.take_action(self.cur_state, self.omega)\n",
    "        (obs, rwd, stop, _) = self.env.step(act)\n",
    "        obs = cast(Observation, obs)\n",
    "\n",
    "        self.episode.append((self.cur_state, act, rwd))\n",
    "\n",
    "        self.cur_state = obs\n",
    "\n",
    "        self.ready_act = self.algm.take_action(self.cur_state, self.omega)\n",
    "\n",
    "        self.algm.after_step((self.cur_state, self.ready_act), self.episode, self.omega)\n",
    "\n",
    "        if stop:\n",
    "            self.episode.append((self.cur_state, None, None))\n",
    "            # self.episodes.append(self.episode)\n",
    "            self.end = True\n",
    "            self.algm.on_termination(self.episode, self.omega)\n",
    "            # self.episode = []\n",
    "            return (obs, stop, self.episode)\n",
    "\n",
    "        return (obs, stop, None)\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.clear()\n",
    "        self.env.close()\n",
    "\n",
    "    def predict(self, s: State) -> float:\n",
    "        return np.max(\n",
    "            [\n",
    "                self.algm.predict((s, a), self.omega)\n",
    "                for a in self.algm.allowed_actions(s)\n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 200/500 [29:18<43:58,  8.79s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/diyuan/openai-gym/aoc.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000008?line=21'>22</a>\u001b[0m end \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000008?line=22'>23</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m end:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000008?line=23'>24</a>\u001b[0m     _, end, episode \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000008?line=25'>26</a>\u001b[0m     \u001b[39m# agent.render()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000008?line=27'>28</a>\u001b[0m     \u001b[39mif\u001b[39;00m end:\n",
      "\u001b[1;32m/home/diyuan/openai-gym/aoc.ipynb Cell 8'\u001b[0m in \u001b[0;36mAgent.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000007?line=32'>33</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode\u001b[39m.\u001b[39mappend((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_state, act, rwd))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000007?line=34'>35</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_state \u001b[39m=\u001b[39m obs\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000007?line=36'>37</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready_act \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malgm\u001b[39m.\u001b[39;49mtake_action(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcur_state, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49momega)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000007?line=38'>39</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgm\u001b[39m.\u001b[39mafter_step((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_state, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready_act), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39momega)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000007?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m stop:\n",
      "\u001b[1;32m/home/diyuan/openai-gym/aoc.ipynb Cell 7'\u001b[0m in \u001b[0;36mAlgorithmInterface.take_action\u001b[0;34m(self, s, omega)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=218'>219</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_terminal_state(s):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=219'>220</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mallowed_actions(s))\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=221'>222</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_algorithm\u001b[39m.\u001b[39;49mtake_action(s, omega)\n",
      "\u001b[1;32m/home/diyuan/openai-gym/aoc.ipynb Cell 7'\u001b[0m in \u001b[0;36mSigmaGreddy.take_action\u001b[0;34m(self, s, w)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=175'>176</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(all_actions)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=176'>177</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=177'>178</a>\u001b[0m     maxi \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=178'>179</a>\u001b[0m         [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mappx_algorithm\u001b[39m.\u001b[39mpredict((s, a), w) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m all_actions]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=179'>180</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=180'>181</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m all_actions[maxi]\n",
      "\u001b[1;32m/home/diyuan/openai-gym/aoc.ipynb Cell 7'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=175'>176</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(all_actions)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=176'>177</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=177'>178</a>\u001b[0m     maxi \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=178'>179</a>\u001b[0m         [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mappx_algorithm\u001b[39m.\u001b[39;49mpredict((s, a), w) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m all_actions]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=179'>180</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=180'>181</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m all_actions[maxi]\n",
      "\u001b[1;32m/home/diyuan/openai-gym/aoc.ipynb Cell 7'\u001b[0m in \u001b[0;36mLinear.predict\u001b[0;34m(self, sa, w)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=144'>145</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, sa: StateAction, w: Weight) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=145'>146</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39minner(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_algorithm\u001b[39m.\u001b[39;49mto_feature(sa), w)\n",
      "\u001b[1;32m/home/diyuan/openai-gym/aoc.ipynb Cell 7'\u001b[0m in \u001b[0;36mTiling.to_feature\u001b[0;34m(self, sa)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=90'>91</a>\u001b[0m pos_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtilings[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=91'>92</a>\u001b[0m vel_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtilings[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=93'>94</a>\u001b[0m pos_feat \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoint_in_range(pos, t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m pos_dim]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=94'>95</a>\u001b[0m vel_feat \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoint_in_range(vel, t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m vel_dim]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=96'>97</a>\u001b[0m pos_feat_one_dim \u001b[39m=\u001b[39m [inner \u001b[39mfor\u001b[39;00m outer \u001b[39min\u001b[39;00m pos_feat \u001b[39mfor\u001b[39;00m inner \u001b[39min\u001b[39;00m outer]\n",
      "\u001b[1;32m/home/diyuan/openai-gym/aoc.ipynb Cell 7'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=90'>91</a>\u001b[0m pos_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtilings[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=91'>92</a>\u001b[0m vel_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtilings[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=93'>94</a>\u001b[0m pos_feat \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoint_in_range(pos, t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m pos_dim]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=94'>95</a>\u001b[0m vel_feat \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoint_in_range(vel, t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m vel_dim]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=96'>97</a>\u001b[0m pos_feat_one_dim \u001b[39m=\u001b[39m [inner \u001b[39mfor\u001b[39;00m outer \u001b[39min\u001b[39;00m pos_feat \u001b[39mfor\u001b[39;00m inner \u001b[39min\u001b[39;00m outer]\n",
      "\u001b[1;32m/home/diyuan/openai-gym/aoc.ipynb Cell 7'\u001b[0m in \u001b[0;36mTiling.point_in_range\u001b[0;34m(self, p, rang)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=75'>76</a>\u001b[0m v: List[Literal[\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=76'>77</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(rang)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=77'>78</a>\u001b[0m     \u001b[39mif\u001b[39;00m rang[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m p \u001b[39m<\u001b[39m rang[i]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=78'>79</a>\u001b[0m         v\u001b[39m.\u001b[39mappend(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000006?line=79'>80</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TOTAL_TRAINING_EPISODES = 500\n",
    "MAX_EPISODE_STEPS_IN_TRAINING = 2e5\n",
    "env._max_episode_steps= MAX_EPISODE_STEPS_IN_TRAINING\n",
    "agent = Agent(\n",
    "    cast(gym.Env, env),\n",
    "    # Sarsa(2e-2, SigmaGreddy(0.1, Linear(TestFeature())))\n",
    "    # Sarsa(0.3, SigmaGreddy(0.05, Linear(TestFeature())))\n",
    "    Sarsa(0.5/8, SigmaGreddy(0, Linear(Tiling(5, [(-1.2, 0.5), (-0.07, 0.07)]))))\n",
    "    # TDN(9, 2e-4, Linear(), Tiling(5))\n",
    ")\n",
    "old_omega = agent.omega\n",
    "\n",
    "\n",
    "training = tqdm(range(TOTAL_TRAINING_EPISODES))\n",
    "\n",
    "# last_omega: Optional[np.ndarray] = None\n",
    "\n",
    "run_rewards: List[float] = []\n",
    "\n",
    "for run in training:\n",
    "    agent.reset()\n",
    "    end = False\n",
    "    while not end:\n",
    "        _, end, episode = agent.step()\n",
    "\n",
    "        # agent.render()\n",
    "\n",
    "        if end:\n",
    "            run_rewards.append(\n",
    "                np.sum(\n",
    "                    [r if r is not None else 0 for (_, _, r) in cast(Episode, episode)]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # if run > 1:\n",
    "    #     progress.set_postfix_str(\n",
    "    #         str(np.linalg.norm(agent.omega - last_omega)))\n",
    "\n",
    "    # last_omega = deepcopy(agent.omega)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[-1.2, -0.86, -0.52, -0.17999999999999994, 0.15999999999999992, 0.5],\n",
       "  [-0.877,\n",
       "   -0.5369999999999999,\n",
       "   -0.197,\n",
       "   0.14300000000000007,\n",
       "   0.48299999999999993,\n",
       "   0.823],\n",
       "  [-1.523, -1.183, -0.843, -0.5029999999999999, -0.1630000000000001, 0.177],\n",
       "  [-0.5539999999999999,\n",
       "   -0.21399999999999997,\n",
       "   0.126,\n",
       "   0.4660000000000001,\n",
       "   0.8059999999999999,\n",
       "   1.146],\n",
       "  [-1.846, -1.506, -1.166, -0.826, -0.4860000000000001, -0.14600000000000002]],\n",
       " [[-0.07,\n",
       "   -0.042,\n",
       "   -0.013999999999999999,\n",
       "   0.014000000000000012,\n",
       "   0.04200000000000001,\n",
       "   0.07],\n",
       "  [-0.04340000000000001,\n",
       "   -0.0154,\n",
       "   0.012600000000000004,\n",
       "   0.04060000000000001,\n",
       "   0.06860000000000001,\n",
       "   0.0966],\n",
       "  [-0.0966,\n",
       "   -0.06860000000000001,\n",
       "   -0.0406,\n",
       "   -0.01259999999999999,\n",
       "   0.015400000000000007,\n",
       "   0.04340000000000001],\n",
       "  [-0.016800000000000002,\n",
       "   0.011200000000000002,\n",
       "   0.039200000000000006,\n",
       "   0.06720000000000001,\n",
       "   0.0952,\n",
       "   0.1232],\n",
       "  [-0.1232,\n",
       "   -0.0952,\n",
       "   -0.06720000000000001,\n",
       "   -0.03919999999999999,\n",
       "   -0.011199999999999995,\n",
       "   0.016800000000000002]]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.algm.policy_algorithm.appx_algorithm.feature_algorithm.tilings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.algm.policy_algorithm.appx_algorithm.feature_algorithm.to_feature(\n",
    "    ((0.2, 0.05), 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-223.0,\n",
       " -710.0,\n",
       " -476.0,\n",
       " -452.0,\n",
       " -655.0,\n",
       " -819.0,\n",
       " -910.0,\n",
       " -196.0,\n",
       " -608.0,\n",
       " -887.0,\n",
       " -294.0,\n",
       " -657.0,\n",
       " -830.0,\n",
       " -430.0,\n",
       " -277.0,\n",
       " -720.0,\n",
       " -741.0,\n",
       " -687.0,\n",
       " -861.0,\n",
       " -398.0,\n",
       " -222.0,\n",
       " -634.0,\n",
       " -268.0,\n",
       " -483.0,\n",
       " -1328.0,\n",
       " -421.0,\n",
       " -724.0,\n",
       " -477.0,\n",
       " -378.0,\n",
       " -1105.0,\n",
       " -463.0,\n",
       " -387.0,\n",
       " -1053.0,\n",
       " -292.0,\n",
       " -864.0,\n",
       " -259.0,\n",
       " -824.0,\n",
       " -546.0,\n",
       " -481.0,\n",
       " -781.0,\n",
       " -391.0,\n",
       " -734.0,\n",
       " -1144.0,\n",
       " -773.0,\n",
       " -1829.0,\n",
       " -705.0,\n",
       " -703.0,\n",
       " -2070.0,\n",
       " -953.0,\n",
       " -1043.0,\n",
       " -803.0,\n",
       " -673.0,\n",
       " -1537.0,\n",
       " -806.0,\n",
       " -894.0,\n",
       " -1503.0,\n",
       " -2341.0,\n",
       " -880.0,\n",
       " -379.0,\n",
       " -1684.0,\n",
       " -613.0,\n",
       " -441.0,\n",
       " -1740.0,\n",
       " -2601.0,\n",
       " -1136.0,\n",
       " -2813.0,\n",
       " -601.0,\n",
       " -1859.0,\n",
       " -3081.0,\n",
       " -555.0,\n",
       " -1764.0,\n",
       " -714.0,\n",
       " -1009.0,\n",
       " -233.0,\n",
       " -1970.0,\n",
       " -667.0,\n",
       " -875.0,\n",
       " -803.0,\n",
       " -1747.0,\n",
       " -791.0,\n",
       " -373.0,\n",
       " -2210.0,\n",
       " -889.0,\n",
       " -2066.0,\n",
       " -2400.0,\n",
       " -1217.0,\n",
       " -208.0,\n",
       " -1219.0,\n",
       " -1088.0,\n",
       " -1574.0,\n",
       " -859.0,\n",
       " -580.0,\n",
       " -1473.0,\n",
       " -1361.0,\n",
       " -660.0,\n",
       " -664.0,\n",
       " -2063.0,\n",
       " -782.0,\n",
       " -765.0,\n",
       " -1700.0,\n",
       " -1297.0,\n",
       " -898.0,\n",
       " -1853.0,\n",
       " -1206.0,\n",
       " -1274.0,\n",
       " -777.0,\n",
       " -1764.0,\n",
       " -1637.0,\n",
       " -1414.0,\n",
       " -602.0,\n",
       " -950.0,\n",
       " -1023.0,\n",
       " -532.0,\n",
       " -830.0,\n",
       " -370.0,\n",
       " -1986.0,\n",
       " -644.0,\n",
       " -919.0,\n",
       " -3007.0,\n",
       " -1797.0,\n",
       " -2239.0,\n",
       " -703.0,\n",
       " -2732.0,\n",
       " -213.0,\n",
       " -1104.0,\n",
       " -1199.0,\n",
       " -367.0,\n",
       " -890.0,\n",
       " -1403.0,\n",
       " -1093.0,\n",
       " -1006.0,\n",
       " -825.0,\n",
       " -273.0,\n",
       " -2484.0,\n",
       " -1519.0,\n",
       " -1312.0,\n",
       " -560.0,\n",
       " -793.0,\n",
       " -1968.0,\n",
       " -1641.0,\n",
       " -667.0,\n",
       " -2063.0,\n",
       " -558.0,\n",
       " -2062.0,\n",
       " -1270.0,\n",
       " -1880.0,\n",
       " -1260.0,\n",
       " -3413.0,\n",
       " -1615.0,\n",
       " -1970.0,\n",
       " -613.0,\n",
       " -2838.0,\n",
       " -966.0,\n",
       " -1936.0,\n",
       " -741.0,\n",
       " -1348.0,\n",
       " -1754.0,\n",
       " -713.0,\n",
       " -1372.0,\n",
       " -1078.0,\n",
       " -525.0,\n",
       " -3244.0,\n",
       " -404.0,\n",
       " -1291.0,\n",
       " -1173.0,\n",
       " -1323.0,\n",
       " -831.0,\n",
       " -2402.0,\n",
       " -1103.0,\n",
       " -867.0,\n",
       " -1227.0,\n",
       " -1294.0,\n",
       " -1286.0,\n",
       " -1401.0,\n",
       " -866.0,\n",
       " -391.0,\n",
       " -1401.0,\n",
       " -562.0,\n",
       " -1976.0,\n",
       " -3038.0,\n",
       " -1613.0,\n",
       " -1043.0,\n",
       " -1676.0,\n",
       " -2801.0,\n",
       " -1674.0,\n",
       " -1357.0,\n",
       " -1843.0,\n",
       " -876.0,\n",
       " -1296.0,\n",
       " -845.0,\n",
       " -797.0,\n",
       " -2049.0,\n",
       " -367.0,\n",
       " -3980.0,\n",
       " -273.0,\n",
       " -1621.0,\n",
       " -857.0,\n",
       " -1216.0,\n",
       " -1432.0,\n",
       " -1132.0,\n",
       " -953.0,\n",
       " -850.0,\n",
       " -607.0,\n",
       " -3239.0,\n",
       " -514.0,\n",
       " -822.0,\n",
       " -966.0,\n",
       " -643.0,\n",
       " -907.0,\n",
       " -1009.0,\n",
       " -1196.0,\n",
       " -1264.0,\n",
       " -664.0,\n",
       " -1119.0,\n",
       " -553.0,\n",
       " -663.0,\n",
       " -636.0,\n",
       " -1176.0,\n",
       " -1463.0,\n",
       " -2442.0,\n",
       " -1523.0,\n",
       " -3002.0,\n",
       " -983.0,\n",
       " -935.0,\n",
       " -2476.0,\n",
       " -848.0,\n",
       " -688.0,\n",
       " -1175.0,\n",
       " -603.0,\n",
       " -638.0,\n",
       " -1301.0,\n",
       " -1741.0,\n",
       " -801.0,\n",
       " -1222.0,\n",
       " -1493.0,\n",
       " -558.0,\n",
       " -2061.0,\n",
       " -367.0,\n",
       " -2020.0,\n",
       " -2281.0,\n",
       " -1174.0,\n",
       " -1758.0,\n",
       " -743.0,\n",
       " -1117.0,\n",
       " -889.0,\n",
       " -564.0,\n",
       " -1194.0,\n",
       " -1158.0,\n",
       " -3068.0,\n",
       " -1846.0,\n",
       " -1627.0,\n",
       " -1308.0,\n",
       " -1298.0,\n",
       " -1095.0,\n",
       " -2914.0,\n",
       " -2049.0,\n",
       " -5115.0,\n",
       " -2233.0,\n",
       " -1536.0,\n",
       " -1131.0,\n",
       " -616.0,\n",
       " -1387.0,\n",
       " -684.0,\n",
       " -1739.0,\n",
       " -1155.0,\n",
       " -2245.0,\n",
       " -1482.0,\n",
       " -1712.0,\n",
       " -999.0,\n",
       " -640.0,\n",
       " -414.0,\n",
       " -578.0,\n",
       " -1816.0,\n",
       " -185.0,\n",
       " -666.0,\n",
       " -595.0,\n",
       " -811.0,\n",
       " -1390.0,\n",
       " -735.0,\n",
       " -1966.0,\n",
       " -1939.0,\n",
       " -2274.0,\n",
       " -1564.0,\n",
       " -1162.0,\n",
       " -2594.0,\n",
       " -960.0,\n",
       " -826.0,\n",
       " -1631.0,\n",
       " -1226.0,\n",
       " -5711.0,\n",
       " -1059.0,\n",
       " -306.0,\n",
       " -1430.0,\n",
       " -834.0,\n",
       " -860.0,\n",
       " -562.0,\n",
       " -2405.0,\n",
       " -1554.0,\n",
       " -3008.0,\n",
       " -3181.0,\n",
       " -1820.0,\n",
       " -2700.0,\n",
       " -1673.0,\n",
       " -2005.0,\n",
       " -2020.0,\n",
       " -443.0,\n",
       " -488.0,\n",
       " -952.0,\n",
       " -1434.0,\n",
       " -524.0,\n",
       " -2723.0,\n",
       " -1238.0,\n",
       " -662.0,\n",
       " -2698.0,\n",
       " -1463.0,\n",
       " -365.0,\n",
       " -1274.0,\n",
       " -611.0,\n",
       " -443.0,\n",
       " -1233.0,\n",
       " -621.0,\n",
       " -1817.0,\n",
       " -1721.0,\n",
       " -795.0,\n",
       " -2129.0,\n",
       " -1223.0,\n",
       " -2319.0,\n",
       " -916.0,\n",
       " -1974.0,\n",
       " -1437.0,\n",
       " -1942.0,\n",
       " -881.0,\n",
       " -1633.0,\n",
       " -2394.0,\n",
       " -1262.0,\n",
       " -2646.0,\n",
       " -1351.0,\n",
       " -828.0,\n",
       " -1689.0,\n",
       " -722.0,\n",
       " -1345.0,\n",
       " -2183.0,\n",
       " -860.0,\n",
       " -2353.0,\n",
       " -1226.0,\n",
       " -559.0,\n",
       " -1187.0,\n",
       " -895.0,\n",
       " -1128.0,\n",
       " -1049.0,\n",
       " -2244.0,\n",
       " -3286.0,\n",
       " -604.0,\n",
       " -1136.0,\n",
       " -1802.0,\n",
       " -988.0,\n",
       " -1402.0,\n",
       " -614.0,\n",
       " -1639.0,\n",
       " -1726.0,\n",
       " -2187.0,\n",
       " -1260.0,\n",
       " -195.0,\n",
       " -1543.0,\n",
       " -2634.0,\n",
       " -3094.0,\n",
       " -1389.0,\n",
       " -622.0,\n",
       " -547.0,\n",
       " -2231.0,\n",
       " -749.0,\n",
       " -473.0,\n",
       " -1822.0,\n",
       " -2025.0,\n",
       " -2587.0,\n",
       " -557.0,\n",
       " -667.0,\n",
       " -453.0,\n",
       " -949.0,\n",
       " -843.0,\n",
       " -682.0,\n",
       " -1062.0,\n",
       " -519.0,\n",
       " -943.0,\n",
       " -1485.0,\n",
       " -908.0,\n",
       " -2348.0,\n",
       " -996.0,\n",
       " -840.0,\n",
       " -1083.0,\n",
       " -341.0,\n",
       " -754.0,\n",
       " -602.0,\n",
       " -663.0,\n",
       " -957.0,\n",
       " -1320.0,\n",
       " -1406.0,\n",
       " -1397.0,\n",
       " -3699.0,\n",
       " -2344.0,\n",
       " -1201.0,\n",
       " -1542.0,\n",
       " -799.0,\n",
       " -1929.0,\n",
       " -877.0,\n",
       " -283.0,\n",
       " -1410.0,\n",
       " -1835.0,\n",
       " -515.0,\n",
       " -744.0,\n",
       " -2053.0,\n",
       " -1787.0,\n",
       " -1832.0,\n",
       " -223.0,\n",
       " -1496.0,\n",
       " -1053.0,\n",
       " -390.0,\n",
       " -1172.0,\n",
       " -2138.0,\n",
       " -1912.0,\n",
       " -823.0,\n",
       " -1203.0,\n",
       " -904.0,\n",
       " -945.0,\n",
       " -958.0,\n",
       " -824.0,\n",
       " -2819.0,\n",
       " -1700.0,\n",
       " -1890.0,\n",
       " -1887.0,\n",
       " -1551.0,\n",
       " -1222.0,\n",
       " -430.0,\n",
       " -1862.0,\n",
       " -841.0,\n",
       " -1024.0,\n",
       " -793.0,\n",
       " -1781.0,\n",
       " -944.0,\n",
       " -604.0,\n",
       " -1448.0,\n",
       " -308.0,\n",
       " -361.0,\n",
       " -1121.0,\n",
       " -602.0,\n",
       " -274.0,\n",
       " -2835.0,\n",
       " -276.0,\n",
       " -2179.0,\n",
       " -1134.0,\n",
       " -2144.0,\n",
       " -2223.0,\n",
       " -2050.0,\n",
       " -996.0,\n",
       " -1005.0,\n",
       " -1807.0,\n",
       " -2818.0,\n",
       " -2103.0,\n",
       " -3955.0,\n",
       " -2069.0,\n",
       " -818.0,\n",
       " -927.0,\n",
       " -983.0,\n",
       " -1382.0,\n",
       " -1595.0,\n",
       " -1120.0,\n",
       " -894.0,\n",
       " -1253.0,\n",
       " -1070.0,\n",
       " -1697.0,\n",
       " -4048.0,\n",
       " -1815.0,\n",
       " -829.0,\n",
       " -1509.0,\n",
       " -989.0,\n",
       " -809.0,\n",
       " -3060.0,\n",
       " -1450.0,\n",
       " -745.0,\n",
       " -3446.0,\n",
       " -651.0,\n",
       " -1025.0,\n",
       " -1230.0,\n",
       " -2516.0,\n",
       " -888.0,\n",
       " -479.0,\n",
       " -1065.0,\n",
       " -1030.0,\n",
       " -1629.0,\n",
       " -669.0,\n",
       " -879.0,\n",
       " -609.0,\n",
       " -746.0,\n",
       " -1587.0,\n",
       " -2019.0,\n",
       " -1417.0,\n",
       " -985.0,\n",
       " -1721.0,\n",
       " -942.0,\n",
       " -1742.0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[r for r in run_rewards if r > -1 * MAX_EPISODE_STEPS_IN_TRAINING]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -63748.28148018,  -86827.71028157,  -73932.46813441,\n",
       "        -41490.63457065,  -49630.2557875 ,  -14633.46452677,\n",
       "        -33002.69376607,  -16139.92031752,   38134.86211127,\n",
       "         55827.6828243 ,  -23577.70282687,  -47730.07475358,\n",
       "        -50014.92041235,  -51851.0253462 ,  -24214.3902137 ,\n",
       "         -2281.76871578,    8894.487877  ,   66136.04713784,\n",
       "         69512.46430195,       0.        ,       0.        ,\n",
       "         12350.7885832 ,   11288.38932075,    -298.01855473,\n",
       "         -6248.42038339, -106077.82223553,  -93975.87668219,\n",
       "       -115735.10936447,  -64471.90760956,  -90030.56130296,\n",
       "        -32525.97607349,  -38940.3312514 ,   -6992.22734721,\n",
       "        -37719.78614957,       0.        ,       0.        ,\n",
       "        -46747.40898184,  -32446.74643876,  -43807.86054958,\n",
       "        -21265.5315004 ,   -8284.58740632,   25855.5531164 ,\n",
       "          4376.83217312,       0.        ,       0.        ,\n",
       "             0.        ,       0.        ,  -12754.49480765,\n",
       "         -1016.69495997,   -7493.33304289, -172259.37340142,\n",
       "       -166973.81534586, -131058.08844739])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:02<00:11,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success: 188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:13<00:00,  7.44it/s]\n"
     ]
    }
   ],
   "source": [
    "TOTAL_EVALUATE_TIMES = 100\n",
    "success_times = 0\n",
    "MAX_EPISODE_STEPS_IN_EVALUATE = 200\n",
    "env._max_episode_steps= MAX_EPISODE_STEPS_IN_EVALUATE\n",
    "evaluate = tqdm(range(TOTAL_EVALUATE_TIMES))\n",
    "for run in evaluate:\n",
    "    agent.reset()\n",
    "    end = False\n",
    "    i = 0\n",
    "    while not end:\n",
    "        _, end, episode = agent.step()\n",
    "\n",
    "        # agent.render()\n",
    "        i += 1\n",
    "\n",
    "    if i < MAX_EPISODE_STEPS_IN_EVALUATE:\n",
    "        success_times += 1\n",
    "        print(f\"success: {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure()\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(x=[i + 1 for i in range(len(omegas) - 1)],\n",
    "#                y=[np.linalg.norm(omegas[i] - omegas[i-1]) for i in range(1, len(omegas))], mode=\"lines\", name=\"omegas\")\n",
    "# )\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.episodes[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_values = np.load(\"./true_values_arr.npy\", allow_pickle=False)\n",
    "# true_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure()\n",
    "# s = list(range(1000))\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(x=[i + 1 for i in s], y=true_values, mode=\"lines\", name=\"true values\")\n",
    "# )\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[i + 1 for i in s],\n",
    "#         y=[agent.predict(i) for i in s],\n",
    "#         mode=\"lines\",\n",
    "#         name=\"monte-carlo prediction\",\n",
    "#     )\n",
    "# )\n",
    "# fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
