{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from gym.envs.toy_text import BlackjackEnv\n",
    "import gym\n",
    "from typing import Literal, List, Tuple, cast, Dict, Optional, Callable, Protocol, Union\n",
    "import plotly.graph_objects as go\n",
    "from copy import deepcopy\n",
    "from abc import abstractmethod, ABC\n",
    "import math\n",
    "import sys\n",
    "from primefac import primegen\n",
    "from math import floor\n",
    "from tqdm.autonotebook import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<MountainCarEnv<MountainCar-v0>>>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(RANDOM_SEED)\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.589128,  0.      ], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Position = float\n",
    "Velocity = float\n",
    "State = Tuple[Position, Velocity]\n",
    "\n",
    "Action = Literal[0, 1, 2]\n",
    "\n",
    "StateAction = Tuple[State, Action]\n",
    "Observation = State\n",
    "\n",
    "\"\"\"\n",
    "- 0: Accelerate to the Left\n",
    "- 1: Don't accelerate\n",
    "- 2: Accelerate to the Right\n",
    "\"\"\"\n",
    "Reward = float\n",
    "Step = Tuple[State, Optional[Action], Optional[Reward]]\n",
    "Episode = List[Step]\n",
    "\n",
    "# all_states: List[State] = list(range(1000))\n",
    "all_actions: List[Action] = [0, 1, 2]\n",
    "# nums_of_all_state = len(all_states)\n",
    "# nums_of_all_state_action = len(all_states) * len(all_actions)\n",
    "# allowed_actions: List[List[Action]] = [\n",
    "#     all_actions for _ in range(nums_of_all_state)]\n",
    "\n",
    "\n",
    "Feature = np.ndarray\n",
    "Weight = np.ndarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IHT:\n",
    "    \"Structure to handle collisions\"\n",
    "    def __init__(self, size_val):\n",
    "        self.size = size_val\n",
    "        self.overfull_count = 0\n",
    "        self.dictionary = {}\n",
    "\n",
    "    def count(self):\n",
    "        return len(self.dictionary)\n",
    "\n",
    "    def full(self):\n",
    "        return len(self.dictionary) >= self.size\n",
    "\n",
    "    def get_index(self, obj, read_only=False):\n",
    "        d = self.dictionary\n",
    "        if obj in d:\n",
    "            return d[obj]\n",
    "        elif read_only:\n",
    "            return None\n",
    "        size = self.size\n",
    "        count = self.count()\n",
    "        if count >= size:\n",
    "            if self.overfull_count == 0: print('IHT full, starting to allow collisions')\n",
    "            self.overfull_count += 1\n",
    "            return hash(obj) % self.size\n",
    "        else:\n",
    "            d[obj] = count\n",
    "            return count\n",
    "\n",
    "def hash_coords(coordinates, m, read_only=False):\n",
    "    if isinstance(m, IHT): return m.get_index(tuple(coordinates), read_only)\n",
    "    if isinstance(m, int): return hash(tuple(coordinates)) % m\n",
    "    if m is None: return coordinates\n",
    "\n",
    "# active_tiles = tiles(self.hash_table, self.num_of_tilings,\n",
    "#                     [self.position_scale * position, self.velocity_scale * velocity],\n",
    "#                     [action])\n",
    "def tiles(iht_or_size, num_tilings, floats, ints=None, read_only=False):\n",
    "    \"\"\"returns num-tilings tile indices corresponding to the floats and ints\"\"\"\n",
    "    if ints is None:\n",
    "        ints = []\n",
    "    qfloats = [floor(f * num_tilings) for f in floats]\n",
    "    tiles = []\n",
    "    for tiling in range(num_tilings):\n",
    "        tilingX2 = tiling * 2\n",
    "        coords = [tiling]\n",
    "        b = tiling\n",
    "        for q in qfloats:\n",
    "            coords.append((q + b) // num_tilings)\n",
    "            b += tilingX2\n",
    "        coords.extend(ints)\n",
    "        tiles.append(hash_coords(coords, iht_or_size, read_only))\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureInterface(Protocol):\n",
    "    len: int\n",
    "\n",
    "    @abstractmethod\n",
    "    def to_feature(self, sa: StateAction) -> Feature:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def one_hot_encode(self, a: Action) -> List[int]:\n",
    "        assert a in all_actions, f\"bad action encountered: {a}\"\n",
    "        if a == 0:\n",
    "            return [1, 0, 0]\n",
    "        elif a == 1:\n",
    "            return [0, 1, 0]\n",
    "        else:\n",
    "            return [0, 0, 1]\n",
    "\n",
    "\n",
    "POS_MAX = 0.6\n",
    "POS_MIN = -1.2\n",
    "VEL_MAX = .07\n",
    "VEL_MIN = -.07\n",
    "\n",
    "\n",
    "class Tiling2(FeatureInterface):\n",
    "    def __init__(self, num_of_tilings: int, tiles_per_tiling: int, num_of_acts: int):\n",
    "        self.len = (tiles_per_tiling + 1) ** 2 * num_of_tilings * num_of_acts\n",
    "        self.iht = IHT(self.len)\n",
    "        self.num_of_tilising = num_of_tilings\n",
    "        self.tiles_per_tiling = tiles_per_tiling\n",
    "\n",
    "        self.pos_scale = self.tiles_per_tiling / (POS_MAX - POS_MIN)\n",
    "        self.vel_scale = self.tiles_per_tiling / (VEL_MAX - VEL_MIN)\n",
    "\n",
    "    def to_feature(self, sa: StateAction) -> Feature:\n",
    "        ((pos, vel), act) = sa\n",
    "        # f = tiles(self.iht, self.num_of_tilising, [\n",
    "        #           pos * self.pos_scale, vel * self.vel_scale], [act])\n",
    "\n",
    "        f = np.zeros((self.len, ), dtype=int)\n",
    "        tile_indexes = tiles(self.iht, self.num_of_tilising, [\n",
    "                             pos * self.pos_scale, vel * self.vel_scale], [act])\n",
    "        for idx in tile_indexes:\n",
    "            f[idx] = 1\n",
    "\n",
    "        return f\n",
    "\n",
    "\n",
    "class Tiling(FeatureInterface):\n",
    "    def __init__(self, n_tilings: int, input_range: List[Tuple[float, float]]):\n",
    "\n",
    "        if n_tilings // 2 == 0:\n",
    "            n_tilings = n_tilings + 1\n",
    "\n",
    "        assert n_tilings > 1, f\"number of tilings cannot be lower than 2: {n_tilings}\"\n",
    "\n",
    "        self.input_range = input_range\n",
    "        self.dimension = len(input_range)\n",
    "\n",
    "        assert self.dimension >= 1, f\"cannot manupilate on 0-dimension\"\n",
    "\n",
    "        self.n_tilings = n_tilings\n",
    "        # self.n_tiles_per_tiling = n_tilings\n",
    "        # self.len = n_tilings * (2 * n_tilings - 1)\n",
    "        self.len = n_tilings * n_tilings * self.dimension + 3\n",
    "\n",
    "        self.tilings = self.compute_tilings()\n",
    "\n",
    "    def get_tiling_point(self, nums: List[float], n_partitions: int) -> List[float]:\n",
    "        assert len(nums) >= 2, f\"unexpected nums encountered: {nums}\"\n",
    "        assert n_partitions >= 2, f\"unexpected n_partitions encountered: {n_partitions}\"\n",
    "\n",
    "        (l, r) = (nums[0], nums[-1])\n",
    "\n",
    "        delta = (r - l) / n_partitions\n",
    "\n",
    "        return [l, *[l + i * delta for i in range(1, n_partitions)], r]\n",
    "\n",
    "    def find_lowest_prime(self) -> int:\n",
    "        return list(primegen(100 / self.n_tilings))[-1]\n",
    "\n",
    "    def compute_tilings(self) -> List[List[List[float]]]:\n",
    "        points: List[List[List[float]]] = []\n",
    "\n",
    "        for (l, r) in self.input_range:\n",
    "            points_one_dimen: List[List[float]] = []\n",
    "\n",
    "            pivot_points = self.get_tiling_point([l, r], self.n_tilings)\n",
    "            points_one_dimen.append(pivot_points)\n",
    "\n",
    "            move_delta = self.find_lowest_prime()\n",
    "\n",
    "            # print(int(self.n_tilings + 1) / 2)\n",
    "            for i in range(1, int((self.n_tilings + 1) / 2)):\n",
    "                points_one_dimen.append(\n",
    "                    self.move_tiling_points(pivot_points, move_delta * i)\n",
    "                )\n",
    "                points_one_dimen.append(\n",
    "                    self.move_tiling_points(pivot_points, -1 * move_delta * i)\n",
    "                )\n",
    "\n",
    "            points.append(points_one_dimen)\n",
    "\n",
    "        return points\n",
    "\n",
    "    def point_in_range(self, p: float, rang: List[float]) -> List[Literal[0, 1]]:\n",
    "        v: List[Literal[0, 1]] = []\n",
    "        for i in range(1, len(rang)):\n",
    "            if rang[i - 1] <= p < rang[i]:\n",
    "                v.append(1)\n",
    "            else:\n",
    "                v.append(0)\n",
    "\n",
    "        assert (\n",
    "            len(v) == len(rang) - 1\n",
    "        ), f\"bad length of v encountered: {len(v)}, {len(rang)-1}\"\n",
    "\n",
    "        return v\n",
    "\n",
    "    def to_feature(self, sa: StateAction) -> Feature:\n",
    "        ((pos, vel), a) = sa\n",
    "        pos_dim = self.tilings[0]\n",
    "        vel_dim = self.tilings[1]\n",
    "\n",
    "        pos_feat = [self.point_in_range(pos, t) for t in pos_dim]\n",
    "        vel_feat = [self.point_in_range(vel, t) for t in vel_dim]\n",
    "\n",
    "        pos_feat_one_dim = [inner for outer in pos_feat for inner in outer]\n",
    "        vel_feat_one_dim = [inner for outer in vel_feat for inner in outer]\n",
    "\n",
    "        return np.asarray(\n",
    "            [*pos_feat_one_dim, *vel_feat_one_dim, *self.one_hot_encode(a)]\n",
    "        )\n",
    "\n",
    "    def move_tiling_points(\n",
    "        self,\n",
    "        points: List[float],\n",
    "        percentile: int,\n",
    "    ) -> List[float]:\n",
    "        # return [(np.round(lp + delta), np.round(rp + delta)) for (lp, rp) in tiling]\n",
    "        # delta = splitting_points[1] - splitting_points[0]\n",
    "        assert len(points) >= 2, f\"bad points: {points}\"\n",
    "\n",
    "        rang = points[-1] - points[0]\n",
    "\n",
    "        return [p + percentile / 100 * rang for p in points]\n",
    "\n",
    "\n",
    "class TestFeature(FeatureInterface):\n",
    "    def __init__(self):\n",
    "        self.len = 8\n",
    "\n",
    "    def to_feature(self, sa: StateAction) -> Feature:\n",
    "        ((pos, vel), a) = sa\n",
    "        v = [1, pos, vel, pos + vel, pos * vel, *self.one_hot_encode(a)]\n",
    "        assert len(v) == self.len, f\"unexpected length encountered: {len(v)}\"\n",
    "        return np.asarray(v)\n",
    "\n",
    "\n",
    "class AppxInterface(Protocol):\n",
    "    feature_algorithm: FeatureInterface\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, sa: StateAction, w: Weight) -> float:\n",
    "        raise NotImplemented()\n",
    "\n",
    "    @abstractmethod\n",
    "    def gradient(self, sa: StateAction, w: Weight) -> np.ndarray:\n",
    "        raise NotImplemented()\n",
    "\n",
    "\n",
    "class Linear(AppxInterface):\n",
    "    def __init__(self, feature_algorithm: FeatureInterface):\n",
    "        self.feature_algorithm = feature_algorithm\n",
    "\n",
    "    def predict(self, sa: StateAction, w: Weight) -> float:\n",
    "        return np.inner(self.feature_algorithm.to_feature(sa), w)\n",
    "\n",
    "    def gradient(self, sa: StateAction, w: Weight) -> np.ndarray:\n",
    "        return self.feature_algorithm.to_feature(sa)\n",
    "\n",
    "\n",
    "class PolicyInterface(Protocol):\n",
    "    appx_algorithm: AppxInterface\n",
    "\n",
    "    @abstractmethod\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def take_action(self, s: State, w: Weight) -> Action:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class SigmaGreddy(PolicyInterface):\n",
    "    def __init__(self, sigma: float, appx_algorithm: AppxInterface):\n",
    "        self.sigma = sigma\n",
    "        self.appx_algorithm = appx_algorithm\n",
    "\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        return all_actions\n",
    "\n",
    "    def take_action(self, s: State, w: Weight) -> Action:\n",
    "        rand = np.random.random()\n",
    "        all_actions = self.allowed_actions(s)\n",
    "        if rand < self.sigma:\n",
    "            return np.random.choice(all_actions)\n",
    "        else:\n",
    "            maxi = np.argmax(\n",
    "                [self.appx_algorithm.predict((s, a), w) for a in all_actions]\n",
    "            )\n",
    "            return all_actions[maxi]\n",
    "\n",
    "\n",
    "class AlwaysRight(PolicyInterface):\n",
    "    def __init__(self, appx_algorithm: AppxInterface):\n",
    "        self.appx_algorithm = appx_algorithm\n",
    "\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        return all_actions\n",
    "\n",
    "    def take_action(self, s: State, w: Weight) -> Action:\n",
    "        return 2\n",
    "\n",
    "\n",
    "class AlgorithmInterface(Protocol):\n",
    "    n_of_omega: int\n",
    "    policy_algorithm: PolicyInterface\n",
    "\n",
    "    @abstractmethod\n",
    "    def after_step(\n",
    "        self, cur_state_action: StateAction, episode: Episode, omega: np.ndarray\n",
    "    ):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_termination(self, episode: Episode, omega: np.ndarray):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        return self.policy_algorithm.allowed_actions(s)\n",
    "\n",
    "    def is_terminal_state(self, s: State) -> bool:\n",
    "        (pos, _) = s\n",
    "        if pos >= 0.5:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def take_action(self, s: State, omega: Weight) -> Action:\n",
    "        if self.is_terminal_state(s):\n",
    "            return np.random.choice(self.allowed_actions(s))\n",
    "\n",
    "        return self.policy_algorithm.take_action(s, omega)\n",
    "\n",
    "    def predict(self, sa: StateAction, w: Weight) -> float:\n",
    "        # assert -1 <= s <= len(all_states), f\"unexpected state encounter: {s}\"\n",
    "        (s, _) = sa\n",
    "        if self.is_terminal_state(s):\n",
    "            return 0\n",
    "\n",
    "        # if s == -1 or s == len(all_states):\n",
    "        #     return 0\n",
    "\n",
    "        return self.policy_algorithm.appx_algorithm.predict(sa, w)\n",
    "\n",
    "    def gradient(self, sa: StateAction, w: Weight) -> np.ndarray:\n",
    "        # assert 0 <= s < len(all_states), f\"unexpected state encounter: {s}\"\n",
    "\n",
    "        return self.policy_algorithm.appx_algorithm.gradient(sa, w)\n",
    "\n",
    "\n",
    "class Sarsa(AlgorithmInterface):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float,\n",
    "        policy_algorithm=PolicyInterface,\n",
    "        gamma: float = 1.0,\n",
    "    ):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.policy_algorithm = policy_algorithm\n",
    "\n",
    "        self.n_of_omega = self.policy_algorithm.appx_algorithm.feature_algorithm.len\n",
    "\n",
    "    def after_step(\n",
    "        self, this_state_action: StateAction, episode: Episode, omega: np.ndarray\n",
    "    ):\n",
    "        # (this_s, this_a) = this_state_action\n",
    "        history = episode[-1:]\n",
    "        gamma = self.gamma\n",
    "\n",
    "        # if len(history) != (1 + n):\n",
    "        #     return\n",
    "        assert len(history) == (\n",
    "            1\n",
    "        ), f\"unexpected history length encountered: {len(history)}\"\n",
    "\n",
    "        (old_s, old_a, r) = history[0]\n",
    "\n",
    "        rwd = cast(Reward, r) + gamma * self.predict(this_state_action, omega)\n",
    "\n",
    "        omega += (\n",
    "            self.alpha\n",
    "            * (rwd - self.predict((old_s, cast(Action, old_a)), omega))\n",
    "            * self.gradient((old_s, cast(Action, old_a)), omega)\n",
    "        )\n",
    "\n",
    "    def on_termination(self, episode: Episode, omega: Weight):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        algm: AlgorithmInterface,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.algm = algm\n",
    "        self.clear()\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_state: State = self.env.reset()\n",
    "        self.ready_act: Optional[Action] = None\n",
    "        self.end = False\n",
    "        self.episode: Episode = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.reset()\n",
    "\n",
    "        self.omega = np.asarray(\n",
    "            # [np.random.random() for _ in range(self.algm.n_of_omega)]\n",
    "            [0.0 for _ in range(self.algm.n_of_omega)]\n",
    "        )\n",
    "        # self.episodes: List[Episode] = []\n",
    "\n",
    "    def step(self) -> Tuple[Observation, bool, Optional[Episode]]:\n",
    "        assert not self.end, \"cannot step on a ended agent\"\n",
    "\n",
    "        act = self.ready_act or self.algm.take_action(self.cur_state, self.omega)\n",
    "        (obs, rwd, stop, _) = self.env.step(act)\n",
    "        obs = cast(Observation, obs)\n",
    "\n",
    "        self.episode.append((self.cur_state, act, rwd))\n",
    "\n",
    "        self.cur_state = obs\n",
    "\n",
    "        self.ready_act = self.algm.take_action(self.cur_state, self.omega)\n",
    "\n",
    "        self.algm.after_step((self.cur_state, self.ready_act), self.episode, self.omega)\n",
    "\n",
    "        if stop:\n",
    "            self.episode.append((self.cur_state, None, None))\n",
    "            # self.episodes.append(self.episode)\n",
    "            self.end = True\n",
    "            self.algm.on_termination(self.episode, self.omega)\n",
    "            # self.episode = []\n",
    "            return (obs, stop, self.episode)\n",
    "\n",
    "        return (obs, stop, None)\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.clear()\n",
    "        self.env.close()\n",
    "\n",
    "    def predict(self, s: State) -> float:\n",
    "        return np.max(\n",
    "            [\n",
    "                self.algm.predict((s, a), self.omega)\n",
    "                for a in self.algm.allowed_actions(s)\n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:16<00:00, 39.06it/s]\n"
     ]
    }
   ],
   "source": [
    "TOTAL_TRAINING_EPISODES = int(1e4)\n",
    "MAX_EPISODE_STEPS_IN_TRAINING = 1e3\n",
    "env._max_episode_steps = MAX_EPISODE_STEPS_IN_TRAINING\n",
    "agent = Agent(\n",
    "    cast(gym.Env, env),\n",
    "    # Sarsa(2e-2, SigmaGreddy(0.1, Linear(TestFeature())))\n",
    "    # Sarsa(0.3, SigmaGreddy(0.05, Linear(TestFeature())))\n",
    "    # Sarsa(0.5/8, SigmaGreddy(0, Linear(Tiling(5, [(-1.2, 0.5), (-0.07, 0.07)]))))\n",
    "    Sarsa(0.5/8, SigmaGreddy(0, Linear(Tiling2(8, 16, 3))))\n",
    "    # TDN(9, 2e-4, Linear(), Tiling(5))\n",
    ")\n",
    "\n",
    "\n",
    "training = tqdm(range(TOTAL_TRAINING_EPISODES))\n",
    "\n",
    "# last_omega: Optional[np.ndarray] = None\n",
    "\n",
    "run_rewards: List[float] = []\n",
    "\n",
    "for run in training:\n",
    "    agent.reset()\n",
    "    end = False\n",
    "    while not end:\n",
    "        _, end, episode = agent.step()\n",
    "\n",
    "        # agent.render()\n",
    "\n",
    "        if end:\n",
    "            run_rewards.append(\n",
    "                np.sum(\n",
    "                    [r if r is not None else 0 for (\n",
    "                        _, _, r) in cast(Episode, episode)]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # if run > 1:\n",
    "    #     progress.set_postfix_str(\n",
    "    #         str(np.linalg.norm(agent.omega - last_omega)))\n",
    "\n",
    "    # last_omega = deepcopy(agent.omega)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-105.7"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([r for r in run_rewards if r > -1 * MAX_EPISODE_STEPS_IN_TRAINING][-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:09<07:56,  4.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/diyuan/openai-gym/aoc.ipynb Cell 12'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000013?line=8'>9</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m end:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000013?line=9'>10</a>\u001b[0m     _, end, episode \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000013?line=11'>12</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mrender()\n",
      "\u001b[1;32m/home/diyuan/openai-gym/aoc.ipynb Cell 9'\u001b[0m in \u001b[0;36mAgent.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000007?line=50'>51</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/diyuan/openai-gym/aoc.ipynb#ch0000007?line=51'>52</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.9/site-packages/gym/core.py?line=293'>294</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> <a href='file:///~/.local/lib/python3.9/site-packages/gym/core.py?line=294'>295</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py:168\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py?line=162'>163</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcartrans\u001b[39m.\u001b[39mset_translation(\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py?line=163'>164</a>\u001b[0m     (pos \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_position) \u001b[39m*\u001b[39m scale, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_height(pos) \u001b[39m*\u001b[39m scale\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py?line=164'>165</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py?line=165'>166</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcartrans\u001b[39m.\u001b[39mset_rotation(math\u001b[39m.\u001b[39mcos(\u001b[39m3\u001b[39m \u001b[39m*\u001b[39m pos))\n\u001b[0;32m--> <a href='file:///~/.local/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py?line=167'>168</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mviewer\u001b[39m.\u001b[39;49mrender(return_rgb_array\u001b[39m=\u001b[39;49mmode \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mrgb_array\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gym/envs/classic_control/rendering.py:123\u001b[0m, in \u001b[0;36mViewer.render\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.9/site-packages/gym/envs/classic_control/rendering.py?line=120'>121</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, return_rgb_array\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.9/site-packages/gym/envs/classic_control/rendering.py?line=121'>122</a>\u001b[0m     glClearColor(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> <a href='file:///~/.local/lib/python3.9/site-packages/gym/envs/classic_control/rendering.py?line=122'>123</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow\u001b[39m.\u001b[39;49mclear()\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.9/site-packages/gym/envs/classic_control/rendering.py?line=123'>124</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow\u001b[39m.\u001b[39mswitch_to()\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.9/site-packages/gym/envs/classic_control/rendering.py?line=124'>125</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow\u001b[39m.\u001b[39mdispatch_events()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyglet/window/__init__.py:1348\u001b[0m, in \u001b[0;36mBaseWindow.clear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.9/site-packages/pyglet/window/__init__.py?line=1341'>1342</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclear\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.9/site-packages/pyglet/window/__init__.py?line=1342'>1343</a>\u001b[0m     \u001b[39m\"\"\"Clear the window.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.9/site-packages/pyglet/window/__init__.py?line=1343'>1344</a>\u001b[0m \n\u001b[1;32m   <a href='file:///~/.local/lib/python3.9/site-packages/pyglet/window/__init__.py?line=1344'>1345</a>\u001b[0m \u001b[39m    This is a convenience method for clearing the color and depth\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.9/site-packages/pyglet/window/__init__.py?line=1345'>1346</a>\u001b[0m \u001b[39m    buffer.  The window must be the active context (see `switch_to`).\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.9/site-packages/pyglet/window/__init__.py?line=1346'>1347</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///~/.local/lib/python3.9/site-packages/pyglet/window/__init__.py?line=1347'>1348</a>\u001b[0m     gl\u001b[39m.\u001b[39;49mglClear(gl\u001b[39m.\u001b[39;49mGL_COLOR_BUFFER_BIT \u001b[39m|\u001b[39;49m gl\u001b[39m.\u001b[39;49mGL_DEPTH_BUFFER_BIT)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyglet/gl/lib.py:87\u001b[0m, in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.9/site-packages/pyglet/gl/lib.py?line=82'>83</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mGLException\u001b[39;00m(\u001b[39mException\u001b[39;00m):\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.9/site-packages/pyglet/gl/lib.py?line=83'>84</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> <a href='file:///~/.local/lib/python3.9/site-packages/pyglet/gl/lib.py?line=86'>87</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merrcheck\u001b[39m(result, func, arguments):\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.9/site-packages/pyglet/gl/lib.py?line=87'>88</a>\u001b[0m     \u001b[39mif\u001b[39;00m _debug_gl_trace:\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.9/site-packages/pyglet/gl/lib.py?line=88'>89</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TOTAL_EVALUATE_TIMES = 100\n",
    "MAX_EPISODE_STEPS_IN_EVALUATE = 200\n",
    "env._max_episode_steps= MAX_EPISODE_STEPS_IN_EVALUATE\n",
    "evaluate = tqdm(range(TOTAL_EVALUATE_TIMES))\n",
    "for run in evaluate:\n",
    "    agent.reset()\n",
    "    end = False\n",
    "    # i = 0\n",
    "    while not end:\n",
    "        _, end, episode = agent.step()\n",
    "\n",
    "        agent.render()\n",
    "        # i += 1\n",
    "\n",
    "    # if i < MAX_EPISODE_STEPS_IN_EVALUATE:\n",
    "    #     success_times += 1\n",
    "    #     print(f\"success: {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure()\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(x=[i + 1 for i in range(len(omegas) - 1)],\n",
    "#                y=[np.linalg.norm(omegas[i] - omegas[i-1]) for i in range(1, len(omegas))], mode=\"lines\", name=\"omegas\")\n",
    "# )\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.episodes[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_values = np.load(\"./true_values_arr.npy\", allow_pickle=False)\n",
    "# true_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure()\n",
    "# s = list(range(1000))\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(x=[i + 1 for i in s], y=true_values, mode=\"lines\", name=\"true values\")\n",
    "# )\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[i + 1 for i in s],\n",
    "#         y=[agent.predict(i) for i in s],\n",
    "#         mode=\"lines\",\n",
    "#         name=\"monte-carlo prediction\",\n",
    "#     )\n",
    "# )\n",
    "# fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
