{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from gym.envs.toy_text import BlackjackEnv\n",
    "import gym\n",
    "from typing import Literal, List, Tuple, cast, Dict, Optional, Callable, Protocol, Union\n",
    "import plotly.graph_objects as go\n",
    "from copy import deepcopy\n",
    "from abc import abstractmethod, ABC\n",
    "import math\n",
    "import sys\n",
    "from primefac import primegen\n",
    "from tqdm.autonotebook import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<MountainCarEnv<MountainCar-v0>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "# hack the \"private\" variable for training\n",
    "env._max_episode_steps= 2e5\n",
    "env.seed(RANDOM_SEED)\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.589128,  0.      ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Position = float\n",
    "Velocity = float\n",
    "State = Tuple[Position, Velocity]\n",
    "\n",
    "Action = Literal[0, 1, 2]\n",
    "\n",
    "StateAction = Tuple[State, Action]\n",
    "Observation = State\n",
    "\n",
    "\"\"\"\n",
    "- 0: Accelerate to the Left\n",
    "- 1: Don't accelerate\n",
    "- 2: Accelerate to the Right\n",
    "\"\"\"\n",
    "Reward = float\n",
    "Step = Tuple[State, Optional[Action], Optional[Reward]]\n",
    "Episode = List[Step]\n",
    "\n",
    "# all_states: List[State] = list(range(1000))\n",
    "all_actions: List[Action] = [0, 1, 2]\n",
    "# nums_of_all_state = len(all_states)\n",
    "# nums_of_all_state_action = len(all_states) * len(all_actions)\n",
    "# allowed_actions: List[List[Action]] = [\n",
    "#     all_actions for _ in range(nums_of_all_state)]\n",
    "\n",
    "\n",
    "Feature = np.ndarray\n",
    "Weight = np.ndarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureInterface(Protocol):\n",
    "    len: int\n",
    "\n",
    "    @abstractmethod\n",
    "    def to_feature(self, sa: StateAction) -> Feature:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def one_hot_encode(self, a: Action) -> List[int]:\n",
    "        assert a in all_actions, f\"bad action encountered: {a}\"\n",
    "        if a == 0:\n",
    "            return [1, 0, 0]\n",
    "        elif a == 1:\n",
    "            return [0, 1, 0]\n",
    "        else:\n",
    "            return [0, 0, 1]\n",
    "\n",
    "\n",
    "class Tiling(FeatureInterface):\n",
    "    def __init__(self, n_tilings: int, input_range: List[Tuple[float, float]]):\n",
    "\n",
    "        if n_tilings // 2 == 0:\n",
    "            n_tilings = n_tilings + 1\n",
    "\n",
    "        assert n_tilings > 1, f\"number of tilings cannot be lower than 2: {n_tilings}\"\n",
    "\n",
    "        self.input_range = input_range\n",
    "        self.dimension = len(input_range)\n",
    "\n",
    "        assert self.dimension >= 1, f\"cannot manupilate on 0-dimension\"\n",
    "\n",
    "        self.n_tilings = n_tilings\n",
    "        # self.n_tiles_per_tiling = n_tilings\n",
    "        # self.len = n_tilings * (2 * n_tilings - 1)\n",
    "        self.len = n_tilings * (n_tilings ** self.dimension)\n",
    "\n",
    "        self.tiings = self.compute_tilings()\n",
    "\n",
    "    def get_tiling_point(self, nums: List[float], n_partitions: int) -> List[float]:\n",
    "        assert len(nums) >= 2, f\"unexpected nums encountered: {nums}\"\n",
    "        assert n_partitions >= 2, f\"unexpected n_partitions encountered: {n_partitions}\"\n",
    "\n",
    "        (l, r) = (nums[0], nums[-1])\n",
    "\n",
    "        delta = (r - l) / n_partitions\n",
    "\n",
    "        return [l, *[l + i * delta for i in range(1, n_partitions)], r]\n",
    "\n",
    "    def find_lowest_prime(self) -> int:\n",
    "        return list(primegen(100 / self.n_tilings))[-1]\n",
    "\n",
    "    def compute_tilings(self) -> List[List[List[float]]]:\n",
    "        points: List[List[List[float]]] = []\n",
    "\n",
    "        for (l, r) in self.input_range:\n",
    "            points_one_dimen: List[List[float]] = []\n",
    "\n",
    "            pivot_points = self.get_tiling_point([l, r], self.n_tilings)\n",
    "            points_one_dimen.append(pivot_points)\n",
    "\n",
    "            move_delta = self.find_lowest_prime()\n",
    "            for i in range(1, self.n_tilings):\n",
    "                points_one_dimen.append(\n",
    "                    self.move_tiling_points(pivot_points, move_delta * i)\n",
    "                )\n",
    "                points_one_dimen.append(\n",
    "                    self.move_tiling_points(pivot_points, -1 * move_delta * i)\n",
    "                )\n",
    "\n",
    "            points.append(points_one_dimen)\n",
    "\n",
    "        return points\n",
    "\n",
    "    def to_feature(self, sa: StateAction) -> Feature:\n",
    "        ((pos, vel), a) = sa\n",
    "\n",
    "        v = []\n",
    "        return np.asarray(v)\n",
    "\n",
    "    def move_tiling_points(\n",
    "        self,\n",
    "        points: List[float],\n",
    "        percentile: int,\n",
    "    ) -> List[float]:\n",
    "        # return [(np.round(lp + delta), np.round(rp + delta)) for (lp, rp) in tiling]\n",
    "        # delta = splitting_points[1] - splitting_points[0]\n",
    "        assert len(points) >= 2, f\"bad points: {points}\"\n",
    "\n",
    "        rang = points[-1] - points[0]\n",
    "\n",
    "        return [p + percentile * rang for p in points]\n",
    "\n",
    "\n",
    "class TestFeature(FeatureInterface):\n",
    "    def __init__(self):\n",
    "        self.len = 8\n",
    "\n",
    "    def to_feature(self, sa: StateAction) -> Feature:\n",
    "        ((pos, vel), a) = sa\n",
    "        v = [1, pos, vel, pos + vel, pos * vel, *self.one_hot_encode(a)]\n",
    "        assert len(v) == self.len, f\"unexpected length encountered: {len(v)}\"\n",
    "        return np.asarray(v)\n",
    "\n",
    "\n",
    "class AppxInterface(Protocol):\n",
    "    feature_algorithm: FeatureInterface\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, sa: StateAction, w: Weight) -> float:\n",
    "        raise NotImplemented()\n",
    "\n",
    "    @abstractmethod\n",
    "    def gradient(self, sa: StateAction, w: Weight) -> np.ndarray:\n",
    "        raise NotImplemented()\n",
    "\n",
    "\n",
    "class Linear(AppxInterface):\n",
    "    def __init__(self, feature_algorithm: FeatureInterface):\n",
    "        self.feature_algorithm = feature_algorithm\n",
    "\n",
    "    def predict(self, sa: StateAction, w: Weight) -> float:\n",
    "        return np.inner(self.feature_algorithm.to_feature(sa), w)\n",
    "\n",
    "    def gradient(self, sa: StateAction, w: Weight) -> np.ndarray:\n",
    "        return self.feature_algorithm.to_feature(sa)\n",
    "\n",
    "\n",
    "class PolicyInterface(Protocol):\n",
    "    appx_algorithm: AppxInterface\n",
    "\n",
    "    @abstractmethod\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def take_action(self, s: State, w: Weight) -> Action:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class SigmaGreddy(PolicyInterface):\n",
    "    def __init__(self, sigma: float, appx_algorithm: AppxInterface):\n",
    "        self.sigma = sigma\n",
    "        self.appx_algorithm = appx_algorithm\n",
    "\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        return all_actions\n",
    "\n",
    "    def take_action(self, s: State, w: Weight) -> Action:\n",
    "        rand = np.random.random()\n",
    "        all_actions = self.allowed_actions(s)\n",
    "        if rand < self.sigma:\n",
    "            return np.random.choice(all_actions)\n",
    "        else:\n",
    "            maxi = np.argmax(\n",
    "                [self.appx_algorithm.predict((s, a), w) for a in all_actions]\n",
    "            )\n",
    "            return all_actions[maxi]\n",
    "\n",
    "\n",
    "class AlwaysRight(PolicyInterface):\n",
    "    def __init__(self, appx_algorithm: AppxInterface):\n",
    "        self.appx_algorithm = appx_algorithm\n",
    "\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        return all_actions\n",
    "\n",
    "    def take_action(self, s: State, w: Weight) -> Action:\n",
    "        return 2\n",
    "\n",
    "\n",
    "class AlgorithmInterface(Protocol):\n",
    "    n_of_omega: int\n",
    "    policy_algorithm: PolicyInterface\n",
    "\n",
    "    @abstractmethod\n",
    "    def after_step(\n",
    "        self, cur_state_action: StateAction, episode: Episode, omega: np.ndarray\n",
    "    ):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_termination(self, episode: Episode, omega: np.ndarray):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def allowed_actions(self, s: State) -> List[Action]:\n",
    "        return self.policy_algorithm.allowed_actions(s)\n",
    "\n",
    "    def is_terminal_state(self, s: State) -> bool:\n",
    "        (pos, _) = s\n",
    "        if pos >= 0.5:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def take_action(self, s: State, omega: Weight) -> Action:\n",
    "        if self.is_terminal_state(s):\n",
    "            return np.random.choice(self.allowed_actions(s))\n",
    "\n",
    "        return self.policy_algorithm.take_action(s, omega)\n",
    "\n",
    "    def predict(self, sa: StateAction, w: Weight) -> float:\n",
    "        # assert -1 <= s <= len(all_states), f\"unexpected state encounter: {s}\"\n",
    "        (s, _) = sa\n",
    "        if self.is_terminal_state(s):\n",
    "            return 0\n",
    "\n",
    "        # if s == -1 or s == len(all_states):\n",
    "        #     return 0\n",
    "\n",
    "        return self.policy_algorithm.appx_algorithm.predict(sa, w)\n",
    "\n",
    "    def gradient(self, sa: StateAction, w: Weight) -> np.ndarray:\n",
    "        # assert 0 <= s < len(all_states), f\"unexpected state encounter: {s}\"\n",
    "\n",
    "        return self.policy_algorithm.appx_algorithm.gradient(sa, w)\n",
    "\n",
    "\n",
    "class Sarsa(AlgorithmInterface):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float,\n",
    "        policy_algorithm=PolicyInterface,\n",
    "        gamma: float = 1.0,\n",
    "    ):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.policy_algorithm = policy_algorithm\n",
    "\n",
    "        self.n_of_omega = self.policy_algorithm.appx_algorithm.feature_algorithm.len\n",
    "\n",
    "    def after_step(\n",
    "        self, this_state_action: StateAction, episode: Episode, omega: np.ndarray\n",
    "    ):\n",
    "        # (this_s, this_a) = this_state_action\n",
    "        history = episode[-1:]\n",
    "        gamma = self.gamma\n",
    "\n",
    "        # if len(history) != (1 + n):\n",
    "        #     return\n",
    "        assert len(history) == (\n",
    "            1\n",
    "        ), f\"unexpected history length encountered: {len(history)}\"\n",
    "\n",
    "        (old_s, old_a, r) = history[0]\n",
    "\n",
    "        rwd = cast(Reward, r) + gamma * self.predict(this_state_action, omega)\n",
    "\n",
    "        omega += (\n",
    "            self.alpha\n",
    "            * (rwd - self.predict((old_s, cast(Action, old_a)), omega))\n",
    "            * self.gradient((old_s, cast(Action, old_a)), omega)\n",
    "        )\n",
    "\n",
    "    def on_termination(self, episode: Episode, omega: Weight):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        algm: AlgorithmInterface,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.algm = algm\n",
    "        self.clear()\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_state: State = self.env.reset()\n",
    "        self.ready_act: Optional[Action] = None\n",
    "        self.end = False\n",
    "        self.episode: Episode = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.reset()\n",
    "\n",
    "        self.omega = np.asarray(\n",
    "            # [np.random.random() for _ in range(self.algm.n_of_omega)]\n",
    "            [0.0 for _ in range(self.algm.n_of_omega)]\n",
    "        )\n",
    "        # self.episodes: List[Episode] = []\n",
    "\n",
    "    def step(self) -> Tuple[Observation, bool, Optional[Episode]]:\n",
    "        assert not self.end, \"cannot step on a ended agent\"\n",
    "\n",
    "        act = self.ready_act or self.algm.take_action(self.cur_state, self.omega)\n",
    "        (obs, rwd, stop, _) = self.env.step(act)\n",
    "        obs = cast(Observation, obs)\n",
    "\n",
    "        self.episode.append((self.cur_state, act, rwd))\n",
    "\n",
    "        self.cur_state = obs\n",
    "\n",
    "        self.ready_act = self.algm.take_action(self.cur_state, self.omega)\n",
    "\n",
    "        self.algm.after_step((self.cur_state, self.ready_act), self.episode, self.omega)\n",
    "\n",
    "        if stop:\n",
    "            self.episode.append((self.cur_state, None, None))\n",
    "            # self.episodes.append(self.episode)\n",
    "            self.end = True\n",
    "            self.algm.on_termination(self.episode, self.omega)\n",
    "            # self.episode = []\n",
    "            return (obs, stop, self.episode)\n",
    "\n",
    "        return (obs, stop, None)\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.clear()\n",
    "        self.env.close()\n",
    "\n",
    "    def predict(self, s: State) -> float:\n",
    "        return np.max(\n",
    "            [\n",
    "                self.algm.predict((s, a), self.omega)\n",
    "                for a in self.algm.allowed_actions(s)\n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:46<00:00, 10.82it/s]\n"
     ]
    }
   ],
   "source": [
    "TOTAL_TRAINING_EPISODES = 500\n",
    "\n",
    "agent = Agent(\n",
    "    cast(gym.Env, env),\n",
    "    # Sarsa(2e-2, SigmaGreddy(0.1, Linear(TestFeature())))\n",
    "    Sarsa(0.3, SigmaGreddy(0.05, Linear(TestFeature())))\n",
    "    # TDN(9, 2e-4, Linear(), Tiling(5))\n",
    ")\n",
    "old_omega = agent.omega\n",
    "\n",
    "\n",
    "training = tqdm(range(TOTAL_TRAINING_EPISODES))\n",
    "\n",
    "# last_omega: Optional[np.ndarray] = None\n",
    "\n",
    "run_rewards: List[float] = []\n",
    "\n",
    "for run in training:\n",
    "    agent.reset()\n",
    "    end = False\n",
    "    while not end:\n",
    "        _, end, episode = agent.step()\n",
    "\n",
    "        # agent.render()\n",
    "\n",
    "        if end:\n",
    "            run_rewards.append(np.sum([r if r is not None else 0 for (_, _, r) in cast(Episode, episode)]))\n",
    "\n",
    "\n",
    "\n",
    "    # if run > 1:\n",
    "    #     progress.set_postfix_str(\n",
    "    #         str(np.linalg.norm(agent.omega - last_omega)))\n",
    "\n",
    "    # last_omega = deepcopy(agent.omega)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2066.0,\n",
       " -678.0,\n",
       " -303.0,\n",
       " -621.0,\n",
       " -1577.0,\n",
       " -399.0,\n",
       " -1168.0,\n",
       " -957.0,\n",
       " -600.0,\n",
       " -652.0,\n",
       " -515.0,\n",
       " -1534.0,\n",
       " -1259.0,\n",
       " -757.0,\n",
       " -1137.0,\n",
       " -733.0,\n",
       " -778.0,\n",
       " -885.0,\n",
       " -404.0,\n",
       " -960.0,\n",
       " -579.0,\n",
       " -717.0,\n",
       " -976.0,\n",
       " -1355.0,\n",
       " -558.0,\n",
       " -784.0,\n",
       " -725.0,\n",
       " -1135.0,\n",
       " -1042.0,\n",
       " -551.0,\n",
       " -563.0,\n",
       " -878.0,\n",
       " -531.0,\n",
       " -231.0,\n",
       " -382.0,\n",
       " -483.0,\n",
       " -500.0,\n",
       " -698.0,\n",
       " -542.0,\n",
       " -1166.0,\n",
       " -475.0,\n",
       " -547.0,\n",
       " -1104.0,\n",
       " -565.0,\n",
       " -454.0,\n",
       " -645.0,\n",
       " -673.0,\n",
       " -673.0,\n",
       " -380.0,\n",
       " -725.0,\n",
       " -723.0,\n",
       " -1046.0,\n",
       " -1027.0,\n",
       " -382.0,\n",
       " -855.0,\n",
       " -1106.0,\n",
       " -977.0,\n",
       " -594.0,\n",
       " -1021.0,\n",
       " -940.0,\n",
       " -497.0,\n",
       " -1210.0,\n",
       " -646.0,\n",
       " -547.0,\n",
       " -1420.0,\n",
       " -469.0,\n",
       " -1114.0,\n",
       " -480.0,\n",
       " -1192.0,\n",
       " -1555.0,\n",
       " -672.0,\n",
       " -625.0,\n",
       " -1419.0,\n",
       " -898.0,\n",
       " -779.0,\n",
       " -504.0,\n",
       " -1080.0,\n",
       " -712.0,\n",
       " -311.0,\n",
       " -864.0,\n",
       " -463.0,\n",
       " -499.0,\n",
       " -651.0,\n",
       " -367.0,\n",
       " -970.0,\n",
       " -697.0,\n",
       " -728.0,\n",
       " -403.0,\n",
       " -470.0,\n",
       " -1044.0,\n",
       " -787.0,\n",
       " -782.0,\n",
       " -755.0,\n",
       " -682.0,\n",
       " -741.0,\n",
       " -1046.0,\n",
       " -899.0,\n",
       " -926.0,\n",
       " -845.0,\n",
       " -632.0,\n",
       " -654.0,\n",
       " -407.0,\n",
       " -886.0,\n",
       " -711.0,\n",
       " -1034.0,\n",
       " -555.0,\n",
       " -402.0,\n",
       " -530.0,\n",
       " -633.0,\n",
       " -582.0,\n",
       " -1224.0,\n",
       " -571.0,\n",
       " -538.0,\n",
       " -476.0,\n",
       " -461.0,\n",
       " -1047.0,\n",
       " -1323.0,\n",
       " -665.0,\n",
       " -818.0,\n",
       " -757.0,\n",
       " -711.0,\n",
       " -699.0,\n",
       " -916.0,\n",
       " -724.0,\n",
       " -1001.0,\n",
       " -1327.0,\n",
       " -715.0,\n",
       " -559.0,\n",
       " -814.0,\n",
       " -774.0,\n",
       " -480.0,\n",
       " -377.0,\n",
       " -935.0,\n",
       " -399.0,\n",
       " -629.0,\n",
       " -539.0,\n",
       " -436.0,\n",
       " -808.0,\n",
       " -750.0,\n",
       " -755.0,\n",
       " -556.0,\n",
       " -486.0,\n",
       " -410.0,\n",
       " -826.0,\n",
       " -857.0,\n",
       " -455.0,\n",
       " -888.0,\n",
       " -646.0,\n",
       " -1243.0,\n",
       " -468.0,\n",
       " -496.0,\n",
       " -971.0,\n",
       " -698.0,\n",
       " -650.0,\n",
       " -676.0,\n",
       " -633.0,\n",
       " -829.0,\n",
       " -524.0,\n",
       " -468.0,\n",
       " -809.0,\n",
       " -693.0,\n",
       " -1261.0,\n",
       " -568.0,\n",
       " -967.0,\n",
       " -390.0,\n",
       " -851.0,\n",
       " -542.0,\n",
       " -1223.0,\n",
       " -552.0,\n",
       " -722.0,\n",
       " -1462.0,\n",
       " -663.0,\n",
       " -639.0,\n",
       " -727.0,\n",
       " -625.0,\n",
       " -812.0,\n",
       " -394.0,\n",
       " -651.0,\n",
       " -535.0,\n",
       " -555.0,\n",
       " -783.0,\n",
       " -395.0,\n",
       " -782.0,\n",
       " -1091.0,\n",
       " -568.0,\n",
       " -951.0,\n",
       " -384.0,\n",
       " -801.0,\n",
       " -529.0,\n",
       " -946.0,\n",
       " -1023.0,\n",
       " -740.0,\n",
       " -474.0,\n",
       " -734.0,\n",
       " -448.0,\n",
       " -564.0,\n",
       " -966.0,\n",
       " -567.0,\n",
       " -1022.0,\n",
       " -472.0,\n",
       " -544.0,\n",
       " -805.0,\n",
       " -978.0,\n",
       " -556.0,\n",
       " -892.0,\n",
       " -641.0,\n",
       " -925.0,\n",
       " -383.0,\n",
       " -782.0,\n",
       " -527.0,\n",
       " -616.0,\n",
       " -680.0,\n",
       " -939.0,\n",
       " -621.0,\n",
       " -889.0,\n",
       " -704.0,\n",
       " -796.0,\n",
       " -849.0,\n",
       " -459.0,\n",
       " -1372.0,\n",
       " -876.0,\n",
       " -587.0,\n",
       " -1054.0,\n",
       " -1436.0,\n",
       " -443.0,\n",
       " -936.0,\n",
       " -1085.0,\n",
       " -710.0,\n",
       " -910.0,\n",
       " -817.0,\n",
       " -718.0,\n",
       " -532.0,\n",
       " -560.0,\n",
       " -510.0,\n",
       " -684.0,\n",
       " -961.0,\n",
       " -763.0,\n",
       " -588.0,\n",
       " -760.0,\n",
       " -546.0,\n",
       " -374.0,\n",
       " -840.0,\n",
       " -554.0,\n",
       " -624.0,\n",
       " -1062.0,\n",
       " -688.0,\n",
       " -906.0,\n",
       " -646.0,\n",
       " -791.0,\n",
       " -637.0,\n",
       " -875.0,\n",
       " -764.0,\n",
       " -808.0,\n",
       " -563.0,\n",
       " -1025.0,\n",
       " -457.0,\n",
       " -731.0,\n",
       " -1178.0,\n",
       " -393.0,\n",
       " -624.0,\n",
       " -555.0,\n",
       " -942.0,\n",
       " -729.0,\n",
       " -863.0,\n",
       " -641.0,\n",
       " -883.0,\n",
       " -899.0,\n",
       " -772.0,\n",
       " -666.0,\n",
       " -782.0,\n",
       " -407.0,\n",
       " -498.0,\n",
       " -1389.0,\n",
       " -1109.0,\n",
       " -953.0,\n",
       " -463.0,\n",
       " -1118.0,\n",
       " -616.0,\n",
       " -702.0,\n",
       " -659.0,\n",
       " -1251.0,\n",
       " -705.0,\n",
       " -554.0,\n",
       " -725.0,\n",
       " -402.0,\n",
       " -624.0,\n",
       " -844.0,\n",
       " -475.0,\n",
       " -565.0,\n",
       " -853.0,\n",
       " -403.0,\n",
       " -641.0,\n",
       " -937.0,\n",
       " -802.0,\n",
       " -858.0,\n",
       " -827.0,\n",
       " -1000.0,\n",
       " -880.0,\n",
       " -521.0,\n",
       " -551.0,\n",
       " -796.0,\n",
       " -848.0,\n",
       " -408.0,\n",
       " -662.0,\n",
       " -609.0,\n",
       " -454.0,\n",
       " -1006.0,\n",
       " -1418.0,\n",
       " -641.0,\n",
       " -608.0,\n",
       " -1003.0,\n",
       " -538.0,\n",
       " -558.0,\n",
       " -502.0,\n",
       " -631.0,\n",
       " -520.0,\n",
       " -905.0,\n",
       " -316.0,\n",
       " -967.0,\n",
       " -483.0,\n",
       " -919.0,\n",
       " -614.0,\n",
       " -404.0,\n",
       " -312.0,\n",
       " -1059.0,\n",
       " -709.0,\n",
       " -922.0,\n",
       " -688.0,\n",
       " -563.0,\n",
       " -1356.0,\n",
       " -836.0,\n",
       " -744.0,\n",
       " -813.0,\n",
       " -538.0,\n",
       " -518.0,\n",
       " -855.0,\n",
       " -382.0,\n",
       " -716.0,\n",
       " -583.0,\n",
       " -853.0,\n",
       " -585.0,\n",
       " -604.0,\n",
       " -473.0,\n",
       " -633.0,\n",
       " -554.0,\n",
       " -1038.0,\n",
       " -636.0,\n",
       " -486.0,\n",
       " -488.0,\n",
       " -629.0,\n",
       " -898.0,\n",
       " -443.0,\n",
       " -1044.0,\n",
       " -1107.0,\n",
       " -614.0,\n",
       " -381.0,\n",
       " -866.0,\n",
       " -991.0,\n",
       " -749.0,\n",
       " -618.0,\n",
       " -777.0,\n",
       " -770.0,\n",
       " -444.0,\n",
       " -392.0,\n",
       " -952.0,\n",
       " -1165.0,\n",
       " -960.0,\n",
       " -687.0,\n",
       " -528.0,\n",
       " -497.0,\n",
       " -312.0,\n",
       " -737.0,\n",
       " -688.0,\n",
       " -649.0,\n",
       " -640.0,\n",
       " -704.0,\n",
       " -562.0,\n",
       " -1106.0,\n",
       " -647.0,\n",
       " -558.0,\n",
       " -946.0,\n",
       " -1308.0,\n",
       " -647.0,\n",
       " -1184.0,\n",
       " -719.0,\n",
       " -1043.0,\n",
       " -831.0,\n",
       " -808.0,\n",
       " -1012.0,\n",
       " -577.0,\n",
       " -836.0,\n",
       " -761.0,\n",
       " -1356.0,\n",
       " -489.0,\n",
       " -383.0,\n",
       " -458.0,\n",
       " -1636.0,\n",
       " -709.0,\n",
       " -940.0,\n",
       " -827.0,\n",
       " -738.0,\n",
       " -800.0,\n",
       " -688.0,\n",
       " -821.0,\n",
       " -559.0,\n",
       " -378.0,\n",
       " -930.0,\n",
       " -847.0,\n",
       " -620.0,\n",
       " -562.0,\n",
       " -629.0,\n",
       " -395.0,\n",
       " -999.0,\n",
       " -856.0,\n",
       " -704.0,\n",
       " -588.0,\n",
       " -1054.0,\n",
       " -710.0,\n",
       " -765.0,\n",
       " -471.0,\n",
       " -544.0,\n",
       " -964.0,\n",
       " -696.0,\n",
       " -1080.0,\n",
       " -521.0,\n",
       " -745.0,\n",
       " -806.0,\n",
       " -643.0,\n",
       " -757.0,\n",
       " -388.0,\n",
       " -373.0,\n",
       " -677.0,\n",
       " -736.0,\n",
       " -366.0,\n",
       " -668.0,\n",
       " -624.0,\n",
       " -834.0,\n",
       " -469.0,\n",
       " -689.0,\n",
       " -576.0,\n",
       " -462.0,\n",
       " -463.0,\n",
       " -972.0,\n",
       " -873.0,\n",
       " -655.0,\n",
       " -659.0,\n",
       " -545.0,\n",
       " -1008.0,\n",
       " -540.0,\n",
       " -962.0,\n",
       " -1044.0,\n",
       " -1080.0,\n",
       " -603.0,\n",
       " -467.0,\n",
       " -479.0,\n",
       " -667.0,\n",
       " -1526.0,\n",
       " -482.0,\n",
       " -694.0,\n",
       " -849.0,\n",
       " -1032.0,\n",
       " -634.0,\n",
       " -543.0,\n",
       " -537.0,\n",
       " -610.0,\n",
       " -812.0,\n",
       " -561.0,\n",
       " -759.0,\n",
       " -417.0,\n",
       " -602.0,\n",
       " -1178.0,\n",
       " -915.0,\n",
       " -658.0,\n",
       " -1215.0,\n",
       " -996.0,\n",
       " -383.0,\n",
       " -1021.0,\n",
       " -732.0,\n",
       " -524.0,\n",
       " -886.0,\n",
       " -367.0,\n",
       " -677.0,\n",
       " -1096.0,\n",
       " -945.0,\n",
       " -628.0,\n",
       " -659.0,\n",
       " -1072.0,\n",
       " -561.0,\n",
       " -859.0,\n",
       " -611.0,\n",
       " -542.0,\n",
       " -610.0,\n",
       " -1410.0,\n",
       " -686.0,\n",
       " -669.0,\n",
       " -1246.0,\n",
       " -811.0,\n",
       " -455.0,\n",
       " -1005.0,\n",
       " -627.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[r for r in run_rewards if r > -2e5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.82626824e+42,  1.06813862e+42,  1.19885551e+42,  2.26699414e+42,\n",
       "       -4.60008174e+41, -7.84706506e+41, -9.97220152e+41, -4.43415868e+40])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.75it/s]\n"
     ]
    }
   ],
   "source": [
    "TOTAL_EVALUATE_TIMES = 100\n",
    "success_times = 0\n",
    "evaluate = tqdm(range(TOTAL_EVALUATE_TIMES))\n",
    "for run in evaluate:\n",
    "    agent.reset()\n",
    "    end = False\n",
    "    i = 0\n",
    "    while not end:\n",
    "        _, end, episode = agent.step()\n",
    "\n",
    "        # agent.render()\n",
    "        i += 1\n",
    "\n",
    "    if i < 200:\n",
    "        success_times += 1\n",
    "        print(f\"success: {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure()\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(x=[i + 1 for i in range(len(omegas) - 1)],\n",
    "#                y=[np.linalg.norm(omegas[i] - omegas[i-1]) for i in range(1, len(omegas))], mode=\"lines\", name=\"omegas\")\n",
    "# )\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.episodes[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_values = np.load(\"./true_values_arr.npy\", allow_pickle=False)\n",
    "# true_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure()\n",
    "# s = list(range(1000))\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(x=[i + 1 for i in s], y=true_values, mode=\"lines\", name=\"true values\")\n",
    "# )\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[i + 1 for i in s],\n",
    "#         y=[agent.predict(i) for i in s],\n",
    "#         mode=\"lines\",\n",
    "#         name=\"monte-carlo prediction\",\n",
    "#     )\n",
    "# )\n",
    "# fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
